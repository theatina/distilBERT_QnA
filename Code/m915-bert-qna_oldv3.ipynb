{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n\n# M915 - Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î± ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ ÎšÎµÎ¹Î¼Î­Î½Î¿Ï… \n___\n### Kylafi Christina-Theano <br><br> LT1200012","metadata":{}},{"cell_type":"code","source":"# imports\n# essentials\nimport os\nimport random\nimport numpy as np\nfrom numpy import mean, std\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json\nfrom collections import Counter\nimport re, string, unicodedata\nimport pickle \nfrom datetime import datetime\nimport pytz\nfrom itertools import cycle\nfrom scipy import interp \nimport time\nimport copy\nimport json\nimport csv\nfrom ast import literal_eval\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\n# SKLEARN\nimport sklearn\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import normalize, OneHotEncoder, label_binarize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, log_loss, plot_confusion_matrix, roc_curve, auc, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import utils\nfrom sklearn.svm import SVC\n\n# NLTK\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nnltk.download\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\n\n\n\n# BERT\n# !pip install transformers\n# !pip install pytorch-pretrained-bert\n\nimport transformers\nfrom transformers.data.processors.squad import SquadV2Processor\nfrom transformers.data import squad_convert_examples_to_features\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizer, BertModel, BertForPreTraining, BertTokenizerFast, AdamW, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering, DistilBertTokenizerFast, DistilBertModel\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\nimport warnings\n\n# MORE INSTALLATIONS & IMPORTS\n# !pip install yellowbrick\n# !pip install advertools\n# !pip install vaderSentiment\n# !pip install ekphrasis\n# !pip install tweet-preprocessor\n\nfrom wordcloud import WordCloud\n# import advertools as adv\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# from ekphrasis.classes.segmenter import Segmenter\n# import preprocessor as p\nimport multiprocessing\nfrom shutil import copy\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nfrom IPython.display import Image\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\nfrom IPython.display import FileLink\n\nprint(\"\\nImports Done !\\n\")\n\n# Device settings\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-25T13:13:52.993134Z","iopub.execute_input":"2022-07-25T13:13:52.993595Z","iopub.status.idle":"2022-07-25T13:14:02.635492Z","shell.execute_reply.started":"2022-07-25T13:13:52.993561Z","shell.execute_reply":"2022-07-25T13:14:02.633328Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Functions\ndef squad_df(file_path, record_path=['data','paragraphs','qas','answers']):\n    file = json.loads(open(file_path).read())\n    \n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file,record_path[:-2])\n    \n    # combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    m['context'] = idx\n    data = m[['id','context','question','answers']].set_index('id').reset_index()\n    data['context_id'] = data['context'].factorize()[0]\n    return data\n\n\n# Add answer end index\ndef add_ans_ind(qna_df):\n    texts = qna_df[\"context\"]\n    answers = qna_df[\"answers\"]\n    for row,(text,answers) in enumerate(zip(texts,answers)):\n        for a_num,a in enumerate(answers):\n            a_text = a[\"text\"]\n            a_start = a[\"answer_start\"]\n            a_end = a_start\n            \n            a_end = int(a_start+len(a_text))\n            if text[a_start:a_end] == a_text:\n                qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]=a_end\n            else:\n                if a_start==0:\n                    continue\n                else:\n                    if a_start==1:\n                        if text[a_start-1:a_end-1] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 1\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 1\n\n                    else:\n                        if text[a_start-2:a_end-2] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 2\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 2\n\n           \n    return qna_df\n\n\ndef data_stats(df, type=\"Train\"):\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_token_type_ids=True)\n    enc = \n\n# pr_list = add_ans_ind(qna_train_df_quac)\n# pr_list = add_ans_ind(qna_dev_df_quac)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T13:18:37.936706Z","iopub.execute_input":"2022-07-25T13:18:37.938238Z","iopub.status.idle":"2022-07-25T13:18:37.959711Z","shell.execute_reply.started":"2022-07-25T13:18:37.938191Z","shell.execute_reply":"2022-07-25T13:18:37.958246Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# directories' paths\nsquad_train_path = \"/kaggle/input/bert-code/bert/train-v1.1.json\"\nsquad_dev_path = \"/kaggle/input/bert-code/bert/dev-v1.1.json\"\ntrain_df = squad_df(squad_train_path)\ndev_df = squad_df(squad_dev_path)\n\nresults_path = \"/kaggle/working/results\"\nif not os.path.exists(results_path):\n    os.makedirs(results_path)\n    \neval_v1_path = \"/kaggle/input/bert-code/bert/evaluate.py\"\neval_v2_path = \"/kaggle/input/httpsgithubcomsomiltgbert/evaluate-v2.0.py\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-25T13:18:51.662006Z","iopub.execute_input":"2022-07-25T13:18:51.662446Z","iopub.status.idle":"2022-07-25T13:19:00.208473Z","shell.execute_reply.started":"2022-07-25T13:18:51.662413Z","shell.execute_reply":"2022-07-25T13:19:00.207112Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# clean GPU cache\nimport gc\nimport torch\ndef clean_GPU_cache(print_sum=False):\n  if print_sum:  \n    print(\"\\nBefore\\n\")\n    print(torch.cuda.memory_summary(device=device, abbreviated=False))\n    torch.cuda.memory_summary(device=None, abbreviated=False)\n  torch.cuda.empty_cache()\n  gc.collect()\n  torch.cuda.empty_cache()\n  if print_sum:  \n    print(\"\\n\\nAfter\\n\")\n    print(torch.cuda.memory_summary(device=device, abbreviated=False))\n\ndef check_gpu():\n  clean_GPU_cache()\n  print(\"\\n\")\n  !nvidia-smi\n  print(\"\\n\")\n\nif device==\"cuda\":\n    check_gpu()\n# clean_GPU_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T13:14:22.127100Z","iopub.execute_input":"2022-07-25T13:14:22.127549Z","iopub.status.idle":"2022-07-25T13:14:22.140263Z","shell.execute_reply.started":"2022-07-25T13:14:22.127517Z","shell.execute_reply":"2022-07-25T13:14:22.139117Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load distilBERT tokenizer & model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', return_token_type_ids=True)\n# tokenizer.model_max_length = 250\n\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-25T13:18:18.500262Z","iopub.execute_input":"2022-07-25T13:18:18.500761Z","iopub.status.idle":"2022-07-25T13:18:30.274670Z","shell.execute_reply.started":"2022-07-25T13:18:18.500726Z","shell.execute_reply":"2022-07-25T13:18:30.273458Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# data processing\n# add end offset to the answers dictionaries\ntrain_df=add_ans_ind(train_df)\ndev_df=add_ans_ind(dev_df)\n\n# tokenize\ntrain_encodings = tokenizer(list(train_df['context']), list(train_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\ndev_encodings = tokenizer(list(dev_df['context']), list(dev_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-25T13:19:00.210836Z","iopub.execute_input":"2022-07-25T13:19:00.212349Z","iopub.status.idle":"2022-07-25T13:28:30.120566Z","shell.execute_reply.started":"2022-07-25T13:19:00.212276Z","shell.execute_reply":"2022-07-25T13:28:30.118970Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"type(dev_encodings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols= [ c for c in train_encodings.columns ]\nvalues= [ train_encodings[c] for c in cols ]\ntrain_enc_df=pd.DataFrame(values,columns=cols)\n\ncols= [ c for c in dev_encodings.columns ]\nvalues= [ dev_encodings[c] for c in cols ]\ndev_enc_df=pd.DataFrame(values,columns=cols)\n\ntrain_enc_df.to_csv(os.path.join(results_path,\"SQuADv1_train_encodings.csv\"), index=False, header=True)\ndev_enc_df.to_csv(os.path.join(results_path,\"SQuADv1_dev_encodings.csv\"), index=False, header=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model training\nif device==\"cuda\":\n    check_gpu()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“š References\n\n1. [text]()\n2. [text]()\n3. [text]()\n4. [text]()\n5. [text]()\n6. [NLP - Document Retrieval for Question Answering](https://www.kaggle.com/code/leomauro/nlp-document-retrieval-for-question-answering)\n7. [text]() \n8. [text]()\n9. [text]()\n10. [text]()\n11. [text]()\n12. [text]()\n13. [text]()\n14. [text]()\n15. [text]()\n16. [text]()\n17. [text]()\n\n","metadata":{}},{"cell_type":"code","source":"# eval_set = \"NewsQA\"\n# preds_file = f\"/content/Results/preds_ftune{ftune_set}_eval{eval_set}.json\"\n\n# total_preds = total_preds_list(model,dev_dataset,device,dev_df)\n\n# with open(preds_file, \"w\") as outfile:\n#     json.dump(ast.literal_eval(json.dumps(total_preds)), outfile)\n# !python3 {eval_file} ../input/squad2/dev-v2.0.json preds.json {preds_file} > ./ftune{ftune_set}_eval{eval_set}.txt\n# !python3 {eval_file} ../input/squad2/dev-v2.0.json preds.json {preds_file}","metadata":{"execution":{"iopub.status.busy":"2022-07-24T16:58:55.418626Z","iopub.execute_input":"2022-07-24T16:58:55.419316Z","iopub.status.idle":"2022-07-24T16:58:55.423796Z","shell.execute_reply.started":"2022-07-24T16:58:55.419277Z","shell.execute_reply":"2022-07-24T16:58:55.422732Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}