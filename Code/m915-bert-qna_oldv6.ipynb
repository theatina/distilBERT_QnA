{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n\n# M915 - Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î± ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ ÎšÎµÎ¹Î¼Î­Î½Î¿Ï… \n___\n### Kylafi Christina-Theano <br><br> LT1200012","metadata":{}},{"cell_type":"code","source":"# imports\n# essentials\nimport os\nimport random\nimport numpy as np\nfrom numpy import mean, std\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json\nfrom collections import Counter\nimport re, string, unicodedata\nimport pickle \nfrom datetime import datetime\nimport pytz\nfrom itertools import cycle\nfrom scipy import interp \nimport time\nimport copy\nimport json\nimport csv\nfrom ast import literal_eval\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\n# SKLEARN\nimport sklearn\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import normalize, OneHotEncoder, label_binarize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, log_loss, plot_confusion_matrix, roc_curve, auc, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import utils\nfrom sklearn.svm import SVC\n\n# NLTK\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nnltk.download\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\n\nimport ast\n!pip install datasets\nfrom datasets import load_metric\n\n# BERT\n# !pip install transformers\n# !pip install pytorch-pretrained-bert\n\nimport transformers\nfrom transformers.data.processors.squad import SquadV2Processor\nfrom transformers.data import squad_convert_examples_to_features\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizer, BertModel, BertForPreTraining, BertTokenizerFast, AdamW, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering, DistilBertTokenizerFast, DistilBertModel\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\nimport warnings\n\n# MORE INSTALLATIONS & IMPORTS\n# !pip install yellowbrick\n# !pip install advertools\n# !pip install vaderSentiment\n# !pip install ekphrasis\n# !pip install tweet-preprocessor\n\nfrom wordcloud import WordCloud\n# import advertools as adv\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# from ekphrasis.classes.segmenter import Segmenter\n# import preprocessor as p\nimport multiprocessing\nfrom shutil import copy\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nfrom IPython.display import Image\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\nfrom IPython.display import FileLink\n\nprint(\"\\nImports Done !\\n\")\n\n# Device settings\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:17:47.666837Z","iopub.execute_input":"2022-07-26T00:17:47.667700Z","iopub.status.idle":"2022-07-26T00:18:10.066364Z","shell.execute_reply.started":"2022-07-26T00:17:47.667167Z","shell.execute_reply":"2022-07-26T00:18:10.065227Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Functions\ndef squad_df(file_path, record_path=['data','paragraphs','qas','answers']):\n    file = json.loads(open(file_path).read())\n    \n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file,record_path[:-2])\n    \n    # combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    m['context'] = idx\n    data = m[['id','context','question','answers']].set_index('id').reset_index()\n    data['context_id'] = data['context'].factorize()[0]\n    return data\n\n\n# Add answer end index\ndef add_ans_ind(qna_df):\n    texts = qna_df[\"context\"]\n    answers = qna_df[\"answers\"]\n    for row,(text,answers) in enumerate(zip(texts,answers)):\n        for a_num,a in enumerate(answers):\n            a_text = a[\"text\"]\n            a_start = a[\"answer_start\"]\n            a_end = a_start\n            \n            a_end = int(a_start+len(a_text))\n            if text[a_start:a_end] == a_text:\n                qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]=a_end\n            else:\n                if a_start==0:\n                    continue\n                else:\n                    if a_start==1:\n                        if text[a_start-1:a_end-1] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 1\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 1\n\n                    else:\n                        if text[a_start-2:a_end-2] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 2\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 2\n\n           \n    return qna_df\n\ndef add_token_positions(encodings, texts, starts, ends):\n    # initialize lists to contain the token indices of answer start/end\n    start_pos = []\n    end_pos = []\n    unanswerable_pos = tokenizer.model_max_length\n    for i,(text,start,end) in enumerate( zip(texts,starts,ends) ):\n        # unanswerable questions\n        if start==end==len(text):\n            start_pos.append(unanswerable_pos)\n            end_pos.append(unanswerable_pos)\n            continue\n        else:\n            start_pos.append(encodings.char_to_token(i, start))\n            end_pos.append(encodings.char_to_token(i, end))\n            if start_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n                continue\n           \n            shift = 1\n            while end_pos[-1] is None and end-shift>start:\n                end_pos[-1] = encodings.char_to_token(i, end - shift)\n                shift += 1\n            if end_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n\n    encodings.update({'start_positions': start_pos, 'end_positions': end_pos})\n\n# apply function to our data\n# add_token_positions(dev_encodings, list(qna_dev_df['AnswerStart']), list(qna_dev_df['AnswerEnd']))\n\n\ndef data_stats(df, type=\"Train\"):\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', return_token_type_ids=True)\n    enc = tokenizer(list(df['context']), list(df['question']), max_length=tokenizer.model_max_length )\n    \n\ndef save_json_evalsquad1(total_preds,filepath=\"preds.json\", dev_set=\"/content/dev-v2.0.json\"):\n    with open(filepath, \"w\") as outfile:\n        json.dump(ast.literal_eval(json.dumps(total_preds)), outfile)\n    \n#     !python3 /content/evaluate-v2.0.py {dev_set} {filepath}\n    !python3 /kaggle/input/bert-code/bert/evaluate.py {dev_set} {filepath} \n\n    \ndef get_prediction(qid,total_preds):\n    return total_preds[qid]\n\ndef normalize_text(s):\n    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_exact_match(prediction, truth):\n    return int(normalize_text(prediction) == normalize_text(truth))\n\ndef compute_f1(prediction, truth):\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n    \n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n    \n    common_tokens = set(pred_tokens) & set(truth_tokens)\n    \n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n    \n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n    \n    return 2 * (prec * rec) / (prec + rec)\n\n\n\ndef get_gold_answers(qid,id_to_answers):\n    \n    gold_answers = [answer[\"text\"] for answer in id_to_answers[qid] if answer[\"text\"]]\n\n    # if gold_answers doesn't exist it's because this is a negative example - \n    # the only correct answer is an empty string\n    if not gold_answers:\n        gold_answers = [\"\"]\n    \n    return gold_answers\n\ndef evalEMandF1_squad1(dev_df,total_preds):\n    global id_to_answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n    id_s = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n#     id_to_answers = {k:v for k,v in zip(id_s,answer_s)}\n    em_score = []\n    f1_score = []\n\n#   logfile for predictions and true answers\n    logfile_name = \"best_model_preds.txt\"\n    logfile_path = os.path.join( os.path.join(\"./Results/Logfiles\"),logfile_name)\n    logfile = open(logfile_path, \"w\", encoding=\"utf-8\")\n    logfile.write(\"Best Model Question Predictions on Dev Set\\n\")\n    for num,(qid,quest) in enumerate(zip(id_s,questions)):\n        prediction = get_prediction(qid,total_preds)\n        true_answers = get_gold_answers(qid,id_to_answers)\n\n        em_score.append(max((compute_exact_match(prediction, answer)) for answer in true_answers))\n        f1_score.append(max((compute_f1(prediction, answer)) for answer in true_answers))\n        \n        log_str = f\"\\n\\n{num+1}. Question: {quest}\\nTrue Answer(s): {true_answers}\\nPrediction(s): {prediction}\\nEM: {bool(int(em_score[-1]))}\\nF1: {f1_score[-1]*100:.2f}%\"\n        logfile.write(log_str)\n    \n    logfile.close()\n    \n    em = sum(em_score)/len(id_s)*100\n    f1_mean = sum(f1_score)/len(id_s)*100\n    f1_max = max(f1_score)*100\n    score_str = f\"Dev Set Scores -- \\tEM: {em:.2f}% \\tF1 (mean): {f1_mean:.2f}% \\tF1 (max): {f1_max:.2f}%\\n\"\n    print(score_str)\n    return em,f1_mean,f1_max,score_str\n\n\ndef total_preds_list(model,dev_dataset,device,dev_df):\n    global id_to_answers\n    model.to(device)\n    model.eval()\n    # initialize list to store epoch accuracies\n\n    total_preds = {}\n    acc = []\n    gtruth = {}\n    predictions = {}\n\n    batch_size=64\n    val_loader = DataLoader(dev_dataset, batch_size=batch_size)\n\n    # create a dictionary to link question id's with answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n    id_list = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n    loss_dev = 0\n    loop_dev = tq.tqdm(val_loader,leave=True)\n    for batch,ids in zip(val_loader, id_list):\n        # we don't need to calculate gradients as we're not training\n        with torch.no_grad():\n            # pull batched items from loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            # we will use true positions for accuracy calc\n    #         start_true = batch['start_positions'].to(device)\n    #         end_true = batch['end_positions'].to(device)\n            qid = ids\n            # make predictions\n            outputs = model(input_ids, attention_mask=attention_mask)\n#             loss = outputs[0]\n#             loss_dev += loss.item()\n    #         print(outputs['start_logits'])\n            # pull prediction tensors out and argmax to get predicted tokens\n            start_pred = torch.argmax(outputs['start_logits'], dim=1)\n            end_pred = torch.argmax(outputs['end_logits'], dim=1)\n\n    #         ans_pred_list = [ (question_id,p) for question_id,p in zip(qid,preds) ]\n            preds = [ tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[num][start_pred[num]:end_pred[num]])) for num in range(len(ids)) ]\n            total_preds.update({ q:pred for q,pred in zip(qid,preds) })\n            \n            loop_dev.update(1)\n\n    return total_preds\n\n\n# pr_list = add_ans_ind(qna_train_df_quac)\n# pr_list = add_ans_ind(qna_dev_df_quac)","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:18:10.068761Z","iopub.execute_input":"2022-07-26T00:18:10.069602Z","iopub.status.idle":"2022-07-26T00:18:10.138905Z","shell.execute_reply.started":"2022-07-26T00:18:10.069563Z","shell.execute_reply":"2022-07-26T00:18:10.137593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# directories' paths\nsquad_train_path = \"/kaggle/input/bert-code/bert/train-v1.1.json\"\nsquad_dev_path = \"/kaggle/input/bert-code/bert/dev-v1.1.json\"\ntrain_df = squad_df(squad_train_path)\ndev_df = squad_df(squad_dev_path)\n\nresults_path = \"/kaggle/working/results\"\nif not os.path.exists(results_path):\n    os.makedirs(results_path)\n    \neval_v1_path = \"/kaggle/input/bert-code/bert/evaluate.py\"\neval_v2_path = \"/kaggle/input/httpsgithubcomsomiltgbert/evaluate-v2.0.py\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-26T00:18:10.142103Z","iopub.execute_input":"2022-07-26T00:18:10.142552Z","iopub.status.idle":"2022-07-26T00:18:17.119638Z","shell.execute_reply.started":"2022-07-26T00:18:10.142520Z","shell.execute_reply":"2022-07-26T00:18:17.118550Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# clean GPU cache\nimport gc\nimport torch\ndef clean_GPU_cache(print_sum=False):\n  if print_sum:  \n    print(\"\\nBefore\\n\")\n    print(torch.cuda.memory_summary(device=device, abbreviated=False))\n    torch.cuda.memory_summary(device=None, abbreviated=False)\n  torch.cuda.empty_cache()\n  gc.collect()\n  torch.cuda.empty_cache()\n  if print_sum:  \n    print(\"\\n\\nAfter\\n\")\n    print(torch.cuda.memory_summary(device=device, abbreviated=False))\n\ndef check_gpu():\n  clean_GPU_cache()\n  print(\"\\n\")\n  !nvidia-smi\n  print(\"\\n\")\n\nif device==\"cuda\":\n    check_gpu()\n# clean_GPU_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:18:17.124910Z","iopub.execute_input":"2022-07-26T00:18:17.127365Z","iopub.status.idle":"2022-07-26T00:18:17.140220Z","shell.execute_reply.started":"2022-07-26T00:18:17.127326Z","shell.execute_reply":"2022-07-26T00:18:17.139160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# load distilBERT tokenizer & model\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# tokenizer.model_max_length = 250\n\n# data processing\n# add end offset to the answers dictionaries\ntrain_df=add_ans_ind(train_df)\ndev_df=add_ans_ind(dev_df)\n\n\n# # tokenize\ntrain_encodings = tokenizer(list(train_df['context']), list(train_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\ndev_encodings = tokenizer(list(dev_df['context']), list(dev_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\n\n\n# id to answers dictionary\nglobal id_to_answers\nid_to_answers = { id_num:ans for id_num,ans in zip( dev_df[\"id\"].values, dev_df[\"answers\"].values ) }","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:18:17.141541Z","iopub.execute_input":"2022-07-26T00:18:17.142605Z","iopub.status.idle":"2022-07-26T00:19:16.328591Z","shell.execute_reply.started":"2022-07-26T00:18:17.142568Z","shell.execute_reply":"2022-07-26T00:19:16.327550Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# add_token_positions(train_encodings, qna_train_df[\"Text\"], list(qna_train_df['AnswerStart']), list(qna_train_df['AnswerEnd']))\nanswer_starts_train=[ ans[0][\"answer_start\"] for ans in train_df[\"answers\"].values ]\nanswer_ends_train=[ ans[0][\"answer_end\"] for ans in train_df[\"answers\"].values ]\ntrain_encodings.update({\"start\":answer_starts_train, \"end\":answer_ends_train})\n\nanswer_starts=[ ans[0][\"answer_start\"] for ans in dev_df[\"answers\"].values ]\nanswer_ends=[ ans[0][\"answer_end\"] for ans in dev_df[\"answers\"].values ]\ndev_encodings.update({\"start\":answer_starts, \"end\":answer_ends})\n\nadd_token_positions(train_encodings, train_df[\"context\"], answer_starts_train, answer_ends_train)\n\n# create dataaframes with useful information\ntrain_df=pd.DataFrame({ k:train_encodings[k] for k in train_encodings.keys() })\ndev_df=pd.DataFrame({ k:dev_encodings[k] for k in dev_encodings.keys() })\n\n# save encodings\n# train_df.to_csv(os.path.join(results_path,\"SQuADv1_train_encodings.csv\"), index=False, header=True)\n# dev_df.to_csv(os.path.join(results_path,\"SQuADv1_dev_encodings.csv\"), index=False, header=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:28:57.132390Z","iopub.execute_input":"2022-07-26T00:28:57.133144Z","iopub.status.idle":"2022-07-26T00:29:12.893405Z","shell.execute_reply.started":"2022-07-26T00:28:57.133105Z","shell.execute_reply":"2022-07-26T00:29:12.892429Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# load encodings df\ntrain_enc_path=\"/kaggle/input/squadv1-encodings-bert512/SQuADv1_train_encodings.csv\"\ntrain_encodings=pd.read_csv(train_enc_path)\ndev_enc_path=\"/kaggle/input/squadv1-encodings-bert512/SQuADv1_dev_encodings.csv\"\ndev_encodings=pd.read_csv(dev_enc_path)\n\n# converse to dictionaries\ntrain_encodings={ k:train_encodings[k].values for k in train_encodings.columns  }\ndev_encodings={ k:dev_encodings[k].values for k in dev_encodings.columns  }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTinput(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, ind):\n        return {key: torch.tensor(value[ind]) for key,value in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-26T00:05:34.528544Z","iopub.execute_input":"2022-07-26T00:05:34.528897Z","iopub.status.idle":"2022-07-26T00:05:34.534742Z","shell.execute_reply.started":"2022-07-26T00:05:34.528867Z","shell.execute_reply":"2022-07-26T00:05:34.533812Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nclean_GPU_cache()\ntot_eps = 2\nbatch_size=32\n\n# build datasets for both our training and validation sets\ntrain_dataset = BERTinput(train_encodings)\ndev_dataset = BERTinput(dev_encodings)\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# initialize validation set data loader\nval_loader = DataLoader(dev_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T22:34:14.510974Z","iopub.execute_input":"2022-07-25T22:34:14.511349Z","iopub.status.idle":"2022-07-25T22:34:16.219273Z","shell.execute_reply.started":"2022-07-25T22:34:14.511316Z","shell.execute_reply":"2022-07-25T22:34:16.218191Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# setup GPU/CPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\npretrained = 0\nif pretrained==0:\n    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n    if device==\"cuda\":\n        mode.to_device(device)    \n    model_name = \"DistilBertSQuADv1\"\nelse:\n    model_name = \"DistilBertSQuADv1Pretrained\"\n\n\nmodel.train()\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\nlr = 2e-6\n# optim = torch.optim.Adam(model.parameters(), lr=9e-6)\nweight_decay = 0.999\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n# logfile, datetime_info, model_logfile_dir_path, logfilefullpath = get_file_ptr(dir_path,model_name)\n# log_model_bert(logfile,datetime_info,model_name,optimizer,batch_size,tokenizer.model_max_length,len(train_dataset),len(dev_dataset))\n\noverall_train_loss = []\noverall_dev_loss = []\nmax_f1_mean = -99\nem_scores = []\nf1_scores = []\nep = 0\nloss_dev = []\nfor epoch in range(tot_eps):\n    epoch_loss_train = 0\n    epoch_acc = 0\n    epoch_acc_dev = 0\n    epoch_loss_dev = 0\n    # set model to train mode\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        \n        # initialize calculated gradients (from prev step)\n        optimizer.zero_grad()\n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        # train model on batch and return outputs (incl. loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch+1}')\n        loop.set_postfix(loss=loss.item())\n        loop.update(1)\n        \n        epoch_loss_train += loss.item()\n        \n    \n    \n    ep_loss_train = epoch_loss_train / len(train_loader)\n#     ep_acc_dev = epoch_acc_dev / len(val_loader)\n    ep_loss_dev = epoch_loss_dev / len(val_loader)\n    \n\n    # keeping loss for learning curve plots\n    overall_train_loss.append(ep_loss_train)\n#     overall_dev_loss.append(ep_loss_dev)\n    \n\n    #     evaluation and EM & F1 scores\n    epoch_str = f'\\n----------------------- Epoch {epoch+1} / {tot_eps} -----------------------\\n' \n    print(epoch_str)\n    loss_str = f'Train Loss: {ep_loss_train:.3f}\\n'\n    print(loss_str)  \n    \n    tot_preds = total_preds_list(model,dev_dataset,device,dev_df)\n#     save_json_evalsquad2(tot_preds,filepath=\"preds.json\")\n    em,f1_mean,f1_max,score_str = evalEMandF1_squad1(dev_df,tot_preds)\n    em_scores.append(em)\n    f1_scores.append(f1_mean)\n    \n        # save best model\n    if f1_mean > max_f1_mean:\n        model_path = os.path.join(results_dir,f\"DistilBertSquad1_stateDict_ep{epoch+1}_DevF1{f1_mean:.3f}.dict\")\n        torch.save(model.state_dict(), model_path)\n#         model_path = os.path.join(model_logfile_dir_path,\"BertSquad2.h5\")\n#         torch.save(model, model_path)\n        \n        max_f1_mean = f1_mean\n        ep = epoch\n    \n#     val_str = f'\\t Val. Loss: {epoch_loss_dev:.3f} |  Val. Acc: {ep_acc_dev * 100:.2f}%\\n\\n'\n#     loss_str = f'Train Loss: {ep_loss_train:.3f} | Val. Loss: {ep_loss_dev:.3f} | Val. Acc: {ep_acc_dev * 100:.2f}%\\n\\n'\n    \n    \n#     write to logfile\n    logfile.write(\"\\n\"+epoch_str+loss_str)\n    logfile.write(\"\\n\"+score_str+\"\\n\\n\")\n    \n    \n# torch.save(model.state_dict(), PATH)\nlogfile.close()\nnew_dir_name = f\"{model_logfile_dir_path}_ep{ep+1}__DevF1{f1_scores[ep]:.2f}\"\nos.rename(model_logfile_dir_path,new_dir_name)      ","metadata":{"execution":{"iopub.status.busy":"2022-07-25T23:58:38.134472Z","iopub.execute_input":"2022-07-25T23:58:38.134845Z","iopub.status.idle":"2022-07-25T23:58:44.485514Z","shell.execute_reply.started":"2022-07-25T23:58:38.134811Z","shell.execute_reply":"2022-07-25T23:58:44.484122Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# model training\nif device==\"cuda\":\n    check_gpu()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“š References\n\n1. [text]()\n2. [text]()\n3. [text]()\n4. [text]()\n5. [text]()\n6. [NLP - Document Retrieval for Question Answering](https://www.kaggle.com/code/leomauro/nlp-document-retrieval-for-question-answering)\n7. [text]() \n8. [text]()\n9. [text]()\n10. [text]()\n11. [text]()\n12. [text]()\n13. [text]()\n14. [text]()\n15. [text]()\n16. [text]()\n17. [text]()\n\n","metadata":{}},{"cell_type":"code","source":"# eval_set = \"NewsQA\"\n# preds_file = f\"/content/Results/preds_ftune{ftune_set}_eval{eval_set}.json\"\n\n# total_preds = total_preds_list(model,dev_dataset,device,dev_df)\n\n# with open(preds_file, \"w\") as outfile:\n#     json.dump(ast.literal_eval(json.dumps(total_preds)), outfile)\n# !python3 {eval_file} ../input/squad2/dev-v2.0.json preds.json {preds_file} > ./ftune{ftune_set}_eval{eval_set}.txt\n# !python3 {eval_file} ../input/squad2/dev-v2.0.json preds.json {preds_file}","metadata":{"execution":{"iopub.status.busy":"2022-07-24T16:58:55.418626Z","iopub.execute_input":"2022-07-24T16:58:55.419316Z","iopub.status.idle":"2022-07-24T16:58:55.423796Z","shell.execute_reply.started":"2022-07-24T16:58:55.419277Z","shell.execute_reply":"2022-07-24T16:58:55.422732Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}