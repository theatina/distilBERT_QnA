{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n\n# M915 - Συστήματα Κατανόησης και Παραγωγής Κειμένου <br> <span style=\"font-size:6mm;\"> Assignment 2 </span> <br><br> <span style=\"font-size:5mm;\"> Kylafi Christina-Theano </span> <br> <span style=\"font-size:4mm;\"> LT1200012 </span>\n---\n---","metadata":{}},{"cell_type":"markdown","source":"## Imports\n---","metadata":{}},{"cell_type":"code","source":"# import Python libraries\n\n# essentials\nimport os\nimport random\nfrom random import sample\nimport numpy as np\nfrom numpy import mean, std\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json\nfrom collections import Counter\nimport re, string, unicodedata\nimport pickle \nfrom datetime import datetime\nimport pytz\nfrom itertools import cycle\nfrom scipy import interp \nimport time\nimport copy\nimport json\nimport csv\nfrom ast import literal_eval\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\n# SKLEARN\nimport sklearn\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import normalize, OneHotEncoder, label_binarize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, log_loss, plot_confusion_matrix, roc_curve, auc, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import utils\nfrom sklearn.svm import SVC\n\n# NLTK\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nnltk.download\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n\n!pip install datasets\nfrom datasets import load_metric\n\n# BERT\n# !pip install transformers\n# !pip install pytorch-pretrained-bert\n\nimport transformers\nfrom transformers.data.processors.squad import SquadV2Processor\nfrom transformers.data import squad_convert_examples_to_features\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizer, BertModel, BertForPreTraining, BertTokenizerFast, AdamW, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering, DistilBertTokenizerFast, DistilBertModel\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\nimport warnings\n\n# MORE INSTALLATIONS & IMPORTS\n# !pip install yellowbrick\n# !pip install advertools\n# !pip install vaderSentiment\n# !pip install ekphrasis\n# !pip install tweet-preprocessor\n\nfrom wordcloud import WordCloud\n# import advertools as adv\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# from ekphrasis.classes.segmenter import Segmenter\n# import preprocessor as p\nimport multiprocessing\nfrom shutil import copy\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nfrom IPython.display import Image, FileLink, FileLinks, clear_output\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\n# clear_output()\n\nprint(\"\\nImports Done !\\n\")\n\n# Device settings\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Working on {device}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-27T14:55:12.486655Z","iopub.execute_input":"2022-08-27T14:55:12.487226Z","iopub.status.idle":"2022-08-27T14:55:35.669712Z","shell.execute_reply.started":"2022-08-27T14:55:12.487170Z","shell.execute_reply":"2022-08-27T14:55:35.668218Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.5.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\nImports Done !\n\nWorking on cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Functions\n---","metadata":{}},{"cell_type":"code","source":"# Functions\n\n# convert squad dataset to dataframe by extracting the necessary info\ndef squad_df(file_path, record_path=['data','paragraphs','qas','answers']):\n    file = json.loads(open(file_path).read())\n    \n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file,record_path[:-2])\n    \n    # combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    m['context'] = idx\n    data = m[['id','context','question','answers']].set_index('id').reset_index()\n    # keep the text sequence number for later\n    data['context_id'] = data['context'].factorize()[0]\n    return data\n\n\n# Add answer ending index\ndef add_ans_ind(qna_df):\n    texts = qna_df[\"context\"]\n    answers = qna_df[\"answers\"]\n    for row,(text,answers) in enumerate(zip(texts,answers)):\n        for a_num,a in enumerate(answers):\n            a_text = a[\"text\"]\n            a_start = a[\"answer_start\"]\n            a_end = a_start\n            \n            a_end = int(a_start+len(a_text))\n            if text[a_start:a_end] == a_text:\n                qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]=a_end\n            else:\n                if a_start==0:\n                    continue\n                else:\n                    if a_start==1:\n                        if text[a_start-1:a_end-1] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 1\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 1\n\n                    else:\n                        if text[a_start-2:a_end-2] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 2\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 2\n\n    return qna_df\n\n\n# add the starting-ending positions of answers (token-wise)\ndef add_token_positions(encodings, texts, starts, ends):\n    # initialize lists to contain the token indices of answer start/end\n    start_pos = []\n    end_pos = []\n    unanswerable_pos = tokenizer.model_max_length\n    for i,(text,start,end) in enumerate( zip(texts,starts,ends) ):\n        # unanswerable questions\n        if start==end==len(text):\n            start_pos.append(unanswerable_pos)\n            end_pos.append(unanswerable_pos)\n            continue\n        else:\n            start_pos.append(encodings.char_to_token(i, start))\n            end_pos.append(encodings.char_to_token(i, end))\n            if start_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n                continue\n           \n            shift = 1\n            while end_pos[-1] is None and end-shift>start:\n                end_pos[-1] = encodings.char_to_token(i, end - shift)\n                shift += 1\n            if end_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n\n    encodings.update({'start_positions': start_pos, 'end_positions': end_pos})\n\n# apply function to our data\n# add_token_positions(dev_encodings, list(qna_dev_df['AnswerStart']), list(qna_dev_df['AnswerEnd']))\n\n\ndef save_json_evalsquad1(total_preds,filepath=\"preds.json\", dev_set=\"/content/dev-v2.0.json\"):\n    with open(filepath, \"w\") as outfile:\n        json.dump(ast.literal_eval(json.dumps(total_preds)), outfile)\n    \n#     !python3 /content/evaluate-v2.0.py {dev_set} {filepath}\n    !python3 /kaggle/input/bert-code/bert/evaluate.py {dev_set} {filepath} \n\n    \ndef get_prediction(qid,total_preds):\n    return total_preds[qid]\n\ndef normalize_text(s):\n    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_exact_match(prediction, truth):\n    return int(normalize_text(prediction) == normalize_text(truth))\n\ndef compute_f1(prediction, truth):\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n    \n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n    \n    common_tokens = set(pred_tokens) & set(truth_tokens)\n    \n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n    \n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n    \n    return 2 * (prec * rec) / (prec + rec)\n\n\n\ndef get_gold_answers(qid,id_to_answers):\n    \n    gold_answers = [answer[\"text\"] for answer in id_to_answers[qid] if answer[\"text\"]]\n\n    # if gold_answers doesn't exist it's because this is a negative example - \n    # the only correct answer is an empty string\n    if not gold_answers:\n        gold_answers = [\"\"]\n    \n    return gold_answers\n\ndef evalEMandF1_squad1(dev_df,total_preds,model_dir):\n    global id_to_answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n    id_s = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n#     id_to_answers = {k:v for k,v in zip(id_s,answer_s)}\n    em_score = []\n    f1_score = []\n\n#   logfile for predictions and true answers\n    logfile_name = \"model_preds.txt\"\n    logfile_path = os.path.join( os.path.join(model_dir),logfile_name)\n    logfile = open(logfile_path, \"w\", encoding=\"utf-8\")\n    logfile.write(\"Best Model Question Predictions on Dev Set\\n\")\n    for num,(qid,quest) in enumerate(zip(id_s,questions)):\n        prediction = get_prediction(qid,total_preds)\n        true_answers = get_gold_answers(qid,id_to_answers)\n\n        em_score.append(max((compute_exact_match(prediction, answer)) for answer in true_answers))\n        f1_score.append(max((compute_f1(prediction, answer)) for answer in true_answers))\n        \n        log_str = f\"\\n\\n{num+1}. Question: {quest}\\nTrue Answer(s): {true_answers}\\nPrediction(s): {prediction}\\nEM: {bool(int(em_score[-1]))}\\nF1: {f1_score[-1]*100:.2f}%\"\n        logfile.write(log_str)\n    \n    logfile.close()\n    \n    em = sum(em_score)/len(id_s)*100\n    f1_mean = sum(f1_score)/len(id_s)*100\n    f1_max = max(f1_score)*100\n    score_str = f\"Dev Set Scores -- \\tEM: {em:.2f}% \\tF1 (mean): {f1_mean:.2f}% \\tF1 (max): {f1_max:.2f}%\\n\"\n    print(score_str)\n    return em,f1_mean,f1_max,score_str\n\n\ndef total_preds_list(model,dev_dataset,device,dev_df):\n    global id_to_answers\n    model.to(device)\n    model.eval()\n    # initialize list to store epoch accuracies\n\n    total_preds = {}\n    acc = []\n    gtruth = {}\n    predictions = {}\n\n    batch_size=64\n    val_loader = DataLoader(dev_dataset, batch_size=batch_size)\n    id_list = [ i for i in dev_df[\"id\"].values ]\n    batch_sizes= [ len(val_b[\"input_ids\"]) for val_b in val_loader]\n    batches=len(val_loader)\n    id_loader = [ id_list[i*batch_size:i*batch_size+batch_size] for i in range(batches-1) ]\n    id_loader.append( id_list[- batch_sizes[-1] :] )\n#     print(id_loader, batch_sizes, batches)\n\n    # create a dictionary to link question id's with answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n#     id_list = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n    loss_dev = 0\n    loop_dev = tqdm(val_loader,leave=True)\n    for batch,ids in zip(val_loader, id_loader):\n        # we don't need to calculate gradients as we're not training\n        with torch.no_grad():\n            # pull batched items from loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            # we will use true positions for accuracy calc\n    #         start_true = batch['start_positions'].to(device)\n    #         end_true = batch['end_positions'].to(device)\n            qid = ids\n            # make predictions\n            outputs = model(input_ids, attention_mask=attention_mask)\n#             loss = outputs[0]\n#             loss_dev += loss.item()\n    #         print(outputs['start_logits'])\n            # pull prediction tensors out and argmax to get predicted tokens\n            start_pred = torch.argmax(outputs['start_logits'], dim=1)\n            end_pred = torch.argmax(outputs['end_logits'], dim=1)\n\n    #         ans_pred_list = [ (question_id,p) for question_id,p in zip(qid,preds) ]\n            preds = [ tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[num][start_pred[num]:end_pred[num]])) for num in range(len(qid)) ]\n            total_preds.update({ q:pred for q,pred in zip(qid,preds) })\n            \n            loop_dev.update(1)\n\n    return total_preds\n\n\ndef log_model(logfile,datetime_info,model,model_name,loss_func,optimizer,batch_size,normalization,max_s_len,pretrained):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    # logstring+= f\"\\n\\n\\n\\n{separator}\\n{separator}\\n\\n\"\n    logstring+= f\"{datetime_info} - {model_name}\\n\\n\"\n    logstring+= f\"Features: {model.embedding.embedding_dim}\\n\"\n    logstring+= f\"Max Sentence length: {max_s_len}\\n\"\n    logstring+= f\"Pretrained Embeddings: {pretrained}\\n\"\n    logstring+= f\"Normalization: {normalization}\\n\"\n    # logstring+= f\"Epochs: {epochs}\\n\"\n    logstring+= f\"Batch size: {batch_size}\\n\"\n    logstring+= f\"Optimizer: {optimizer}\\n\"\n    logstring+= f\"Loss function: {loss_func}\"\n    if type(loss_func) == torch.nn.CrossEntropyLoss:\n        logstring+= f\", weight: {loss_func.weight}\\n\"\n    else :\n        logstring+= \"\\n\"\n        logstring+= f\"Layers: {model}\\n\\n\"\n        logstring+= f\"{separator}\\n\\n\"\n\n    logfile.write(logstring)\n\n\ndef log_score(logfile,epoch,model,score_str):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    logstring+= f\"\\n\\n\\n> Epoch:{epoch}\\n\"\n    logstring+= score_str\n\n\n    logfile.write(logstring)\n\n\n# Logfiles\ndef get_file_ptr(drive_path,model_name):\n    tz = pytz.timezone('Europe/Athens')\n    datetime_info = f\"{datetime.now(tz):%d%m%y_%H%M}\"\n\n    logfile_name = f\"{model_name}__{datetime_info}.txt\"\n    model_logfile_dir_path = os.path.join(drive_path, f\"Results/Logfiles/{logfile_name[:-4]}\") \n    if not os.path.exists(model_logfile_dir_path):\n        os.makedirs(model_logfile_dir_path)  \n\n    logfile_path = os.path.join( os.path.join(model_logfile_dir_path),logfile_name)\n    logfile = open(logfile_path, \"w\", encoding=\"utf-8\")\n\n    return logfile, datetime_info, model_logfile_dir_path, logfile_path\n\n\ndef log_model_bert(logfile,datetime_info,model_name,optimizer,batch_size,max_s_len,train_len,val_len):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    # logstring+= f\"\\n\\n\\n\\n{separator}\\n{separator}\\n\\n\"\n    logstring+= f\"{datetime_info} - {model_name}\\n\\n\"\n    logstring+= f\"Max Sentence length: {max_s_len}\\n\"\n    logstring+= f\"Batch size: {batch_size}\\n\"\n    logstring+= f\"Optimizer: {optimizer}\\n\"\n    logstring+= f\"Training Data: {train_len}\\n\"\n    logstring+= f\"Validation Data: {val_len}\\n\"\n    #   logstring+= f\"Layers: {model}\\n\\n\"\n    logstring+= f\"{separator}\\n\\n\"\n\n    logfile.write(logstring)\n    \n  \ndef preprocess_function(examples, tokenizer):\n    questions = [ q.strip() for q in list(examples[\"question\"])]\n    inputs = tokenizer(\n        questions,\n        list(examples[\"context\"]),\n        max_length=tokenizer.model_max_length,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\")\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"].values\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        if type(answers[i])==type(str):\n            answer = eval(answers[i][0])\n        else:\n            answer = answers[i][0]\n        start_char = answer[\"answer_start\"]\n        end_char = start_char + len(answer[\"text\"])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ndef answer_a_question(question,text,tokenizer,model):\n    model.to(device)\n    model.eval()\n        \n    model_in=tokenizer.encode_plus(text,question, return_tensors='pt')\n    in_ids = model_in[\"input_ids\"].to(device)\n    mask = model_in[\"attention_mask\"].to(device)\n    \n    outputs = model(in_ids, attention_mask=mask)\n    start_pred = torch.argmax(outputs['start_logits'], dim=1).item()\n    end_pred = torch.argmax(outputs['end_logits'], dim=1).item()\n    \n    text_ids= model_in[\"input_ids\"][0]\n    text_tokens=tokenizer.convert_ids_to_tokens(text_ids)\n\n    tokens = tokenizer.convert_ids_to_tokens(text_ids[start_pred:end_pred+1])\n    answer = tokenizer.convert_tokens_to_string(tokens)\n    answer = normalize_text(answer).capitalize()\n    \n#     print(f\"\\nText: {text}\\nQuestion: {question}\\n\\nPredicted answer: {answer}\\n\")\n\n    return answer\n\n\n# Data Visualization\ndef data_stats(df, data_type):\n    plt.clf()\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n    enc = tokenizer(list(df['context']), truncation=False )\n    lens=[ len(i_id) for i_id in enc[\"input_ids\"] ]\n    print(max(lens))\n    plt.figure(figsize=(18,12))\n    plt.grid(True)\n    plt.title(f\"{data_type} Set Cumulative Sum\")\n    plt.hist(lens)\n    plt.savefig(os.path.join(results_path,f\"{data_type}_max_lens_Csum.png\"),dpi=300)\n    plt.show()\n\n    return lens\n\ndef bar_plotting_lens(lens, data_type, val_to_show):\n    plt.clf()\n    lens_df=pd.DataFrame(lens)\n    len_desc = [ len for len in lens_df[0].value_counts().index[:val_to_show].tolist() ]\n    len_freq = [ freq for freq in lens_df[0].value_counts().tolist( )[:val_to_show] ]\n    fig = plt.figure(figsize =(48, 6))\n    bar_plot = plt.bar(len_desc, len_freq)\n    max_freq_ind = len_freq.index(max(len_freq))\n    bar_plot[max_freq_ind].set_color('m')\n    plt.xticks(len_desc)\n    plt.xlabel('Length')\n    plt.ylabel('Num of Texts')\n    plt.title(f\"{data_type} set context length\")\n    plt.grid(True)\n    # show plot\n    plt.savefig(os.path.join(results_path,f\"{data_type}_max_lens.png\"),dpi=300)\n    plt.show()\n    return len_desc[max_freq_ind]\n\ndef perc_Csum(lens, data_type):\n    len_vals=[50,100,150,200,256,300,512, max(lens)+1]\n    print(f\"\\nPercentage of {data_type} set context under:\")\n    for L in len_vals:\n        # plot cumulative summary and percentage of samples with context under length L\n         print(f\"{L} -> {sum([i<L for i in lens])/len(lens)*100:.2f}%\")\n    \n\n# pr_list = add_ans_ind(qna_train_df_quac)\n# pr_list = add_ans_ind(qna_dev_df_quac)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T14:55:35.672936Z","iopub.execute_input":"2022-08-27T14:55:35.674011Z","iopub.status.idle":"2022-08-27T14:55:35.779363Z","shell.execute_reply.started":"2022-08-27T14:55:35.673969Z","shell.execute_reply":"2022-08-27T14:55:35.778016Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## SQuADv1.1 Dataset\n---\n\n#### Steps:\n1. Add data \n2. Search for \"bert-code\" (https://www.kaggle.com/datasets/rndmnub/bert-code)\n\nor\n\n1. Save \"train-v1.1.json\" & \"dev-v1.1.json\" \n2. Change the \"squad_train_path\" & \"squad_dev_path\" variables below","metadata":{}},{"cell_type":"code","source":"# directories' paths\nsquad_train_path = \"/kaggle/input/bert-code/bert/train-v1.1.json\"\nsquad_dev_path = \"/kaggle/input/bert-code/bert/dev-v1.1.json\"\ntrain_df = squad_df(squad_train_path)\ndev_df = squad_df(squad_dev_path)\n\n# decrease train dataset for quicker training process\n# train_df = train_df.sample(frac=1).reset_index(drop=True)\n# train_df = train_df[: int(train_df.shape[0]/2)]\n# print(train_df.shape)\n\n# eval_v1_path = \"/kaggle/input/bert-code/bert/evaluate.py\"\n# eval_v2_path = \"/kaggle/input/httpsgithubcomsomiltgbert/evaluate-v2.0.py\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-25T21:37:12.557225Z","iopub.execute_input":"2022-08-25T21:37:12.557570Z","iopub.status.idle":"2022-08-25T21:37:18.640854Z","shell.execute_reply.started":"2022-08-25T21:37:12.557547Z","shell.execute_reply":"2022-08-25T21:37:18.639687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results directory\nresults_path = \"/kaggle/working/results\" #change it if not run in Kaggle environment\nif not os.path.exists(results_path):\n    os.makedirs(results_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T21:33:54.021716Z","iopub.execute_input":"2022-08-25T21:33:54.022156Z","iopub.status.idle":"2022-08-25T21:33:54.027911Z","shell.execute_reply.started":"2022-08-25T21:33:54.022119Z","shell.execute_reply":"2022-08-25T21:33:54.027139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization\n---","metadata":{}},{"cell_type":"code","source":"# plot counts of N_max biggest length values of context in the dataset \nN_max=50\nlens_train = data_stats(train_df, \"Train\")\nbar_plotting_lens(lens_train,\"Train\", N_max)\n\nlens_dev = data_stats(dev_df, \"Dev\")\nbar_plotting_lens(lens_dev,\"Dev\", N_max)\n\n# plot cumulative sum and percentage of samples with context under length L\nperc_Csum(lens_train, \"Train\")\nperc_Csum(lens_dev, \"Dev\")\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T20:28:19.944968Z","iopub.execute_input":"2022-08-25T20:28:19.945214Z","iopub.status.idle":"2022-08-25T20:28:48.510569Z","shell.execute_reply.started":"2022-08-25T20:28:19.945190Z","shell.execute_reply":"2022-08-25T20:28:48.509572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing\n---","metadata":{}},{"cell_type":"code","source":"# clean GPU cache\nimport gc\nimport torch\ndef clean_GPU_cache(print_sum=False):\n    if print_sum:  \n        print(\"\\nBefore\\n\")\n        print(torch.cuda.memory_summary(device=device, abbreviated=False))\n    torch.cuda.memory_summary(device=None, abbreviated=False)\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.empty_cache()\n    if print_sum:  \n        print(\"\\n\\nAfter\\n\")\n        print(torch.cuda.memory_summary(device=device, abbreviated=False))\n\ndef check_gpu():\n    clean_GPU_cache()\n    print(\"\\n\")\n    !nvidia-smi\n    print(\"\\n\")\n\n# clean_GPU_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:40:29.579946Z","iopub.execute_input":"2022-07-28T07:40:29.580518Z","iopub.status.idle":"2022-07-28T07:40:29.592608Z","shell.execute_reply.started":"2022-07-28T07:40:29.580476Z","shell.execute_reply":"2022-07-28T07:40:29.591241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load distilBERT tokenizer & model\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer.model_max_length = 256\n\n# tokenize - truncate or pad only context if needed \ntrain_encodings = tokenizer(list(train_df['context']), list(train_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\ndev_encodings = tokenizer(list(dev_df['context']), list(dev_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\n\n# create the train/test encodings\n# reference: https://huggingface.co/docs/transformers/tasks/question_answering\n# train_encodings = preprocess_function(train_df, tokenizer)\n# dev_encodings = preprocess_function(dev_df, tokenizer)\n\n# id to answers dictionary\nglobal id_to_answers\nid_to_answers = { id_num:ans for id_num,ans in zip( dev_df[\"id\"].values, dev_df[\"answers\"].values ) }","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:40:29.594634Z","iopub.execute_input":"2022-07-28T07:40:29.596456Z","iopub.status.idle":"2022-07-28T07:41:09.539292Z","shell.execute_reply.started":"2022-07-28T07:40:29.596398Z","shell.execute_reply":"2022-07-28T07:41:09.538177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data processing\n# add end offset to the answers dictionaries\ntrain_df=add_ans_ind(train_df)\ndev_df=add_ans_ind(dev_df)\n\nanswer_starts_train=[ ans[0][\"answer_start\"] for ans in train_df[\"answers\"].values ]\nanswer_ends_train=[ ans[0][\"answer_end\"] for ans in train_df[\"answers\"].values ]\ntrain_encodings.update({\"start\":answer_starts_train, \"end\":answer_ends_train})\n\nanswer_starts=[ ans[0][\"answer_start\"] for ans in dev_df[\"answers\"].values ]\nanswer_ends=[ ans[0][\"answer_end\"] for ans in dev_df[\"answers\"].values ]\ndev_encodings.update({\"start\":answer_starts, \"end\":answer_ends})\n\nadd_token_positions(train_encodings, train_df[\"context\"], answer_starts_train, answer_ends_train)\n\n# create dataaframes with useful information\ntrain_enc_df=pd.DataFrame({ k:train_encodings[k] for k in train_encodings.keys() })\ndev_enc_df=pd.DataFrame({ k:dev_encodings[k] for k in dev_encodings.keys() })\n\n# save encodings\n# train_df.to_csv(os.path.join(results_path,f\"SQuADv1_train_encodings_{tokenizer.model_max_length}.csv\"), index=False, header=True)\n# dev_df.to_csv(os.path.join(results_path,f\"SQuADv1_dev_encodings_{tokenizer.model_max_length}.csv\"), index=False, header=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:41:09.540759Z","iopub.execute_input":"2022-07-28T07:41:09.541225Z","iopub.status.idle":"2022-07-28T07:41:11.262055Z","shell.execute_reply.started":"2022-07-28T07:41:09.541187Z","shell.execute_reply":"2022-07-28T07:41:11.260911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load encodings df\n# train_enc_path=f\"/kaggle/input/squadv1-encodings-bert{tokenizer.model_max_length}/SQuADv1_train_encodings_{tokenizer.model_max_length}.csv\"\n# train_encodings=pd.read_csv(train_enc_path)\n# dev_enc_path=f\"/kaggle/input/squadv1-encodings-bert{tokenizer.model_max_length}/SQuADv1_dev_encodings_{tokenizer.model_max_length}.csv\"\n# dev_encodings=pd.read_csv(dev_enc_path)\n\n# # keep half the data due to computational complexity\n# train_encodings=train_encodings[:int(train_encodings.shape[0]/4)]\n# print(train_encodings.shape[0])\n\n# # converse to dictionaries\n# train_encodings={ k:train_encodings[k].values for k in train_encodings.columns  }\n# dev_encodings={ k:dev_encodings[k].values for k in dev_encodings.columns  }","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:53:24.348209Z","iopub.execute_input":"2022-07-27T16:53:24.348589Z","iopub.status.idle":"2022-07-27T16:53:25.627115Z","shell.execute_reply.started":"2022-07-27T16:53:24.348558Z","shell.execute_reply":"2022-07-27T16:53:25.626115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTinput(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, ind):\n        return {key: torch.tensor(value[ind]) for key,value in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:41:11.265337Z","iopub.execute_input":"2022-07-28T07:41:11.265845Z","iopub.status.idle":"2022-07-28T07:41:11.272958Z","shell.execute_reply.started":"2022-07-28T07:41:11.265804Z","shell.execute_reply":"2022-07-28T07:41:11.271770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nclean_GPU_cache()\nbatch_size=32\n\n# build datasets for both our training and validation sets\ntrain_dataset = BERTinput(train_encodings)\ndev_dataset = BERTinput(dev_encodings)\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# initialize validation set data loader\nval_loader = DataLoader(dev_dataset, batch_size=batch_size)\n\n# with open(f'/kaggle/..../train_loader_{tokenizer.model_max_length}.pickle', 'wb') as handle:\n#     pickle.dump(train_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:41:11.274546Z","iopub.execute_input":"2022-07-28T07:41:11.275317Z","iopub.status.idle":"2022-07-28T07:41:11.797208Z","shell.execute_reply.started":"2022-07-28T07:41:11.275280Z","shell.execute_reply":"2022-07-28T07:41:11.795520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n---","metadata":{}},{"cell_type":"code","source":"# model training\n\n# setup GPU/CPU\ncheck_gpu()\n\npretrained = 0\nif pretrained==0:\n    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n    model.to(device)    \n    model_name = \"DistilBertSQuADv1\"\nelse:\n    model_name = \"DistilBertSQuADv1Pretrained\"\n\n\nmodel.train()\ntot_eps = 5\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\nlr = 5e-5\n# optim = torch.optim.Adam(model.parameters(), lr=9e-6)\nweight_decay = 0.999\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\nlogfile, datetime_info, model_logfile_dir_path, logfilefullpath = get_file_ptr(results_path,model_name)\nlog_model_bert(logfile,datetime_info,model_name,optimizer,batch_size,tokenizer.model_max_length,len(train_dataset),len(dev_dataset))\n\noverall_train_loss = []\noverall_dev_loss = []\nmax_f1_mean = -99\nem_scores = []\nf1_scores = []\nep = 0\nloss_dev = []\nfor epoch in range(tot_eps):\n    epoch_loss_train = 0\n    epoch_acc = 0\n    epoch_acc_dev = 0\n    epoch_loss_dev = 0\n    # set model to train mode\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        \n        # initialize calculated gradients (from prev step)\n        optimizer.zero_grad()\n        \n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        \n        # train model on batch and return outputs (including loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        \n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch+1}')\n        loop.set_postfix(loss=loss.item())\n        loop.update(1)\n        \n        epoch_loss_train += loss.item()\n        \n    \n    ep_loss_train = epoch_loss_train / len(train_loader)\n    ep_loss_dev = epoch_loss_dev / len(val_loader)\n\n    # keeping loss for learning curve plots\n    overall_train_loss.append(ep_loss_train)\n\n    #     evaluation and EM & F1 scores\n    epoch_str = f'\\n----------------------- Epoch {epoch+1} / {tot_eps} -----------------------\\n' \n    print(epoch_str)\n    loss_str = f'Train Loss: {ep_loss_train:.3f}\\n'\n    print(loss_str)  \n    \n    tot_preds = total_preds_list(model,dev_dataset,device,dev_df)\n    em,f1_mean,f1_max,score_str = evalEMandF1_squad1(dev_df,tot_preds,model_logfile_dir_path)\n    em_scores.append(em)\n    f1_scores.append(f1_mean)\n    \n        # save best model\n    if f1_mean > max_f1_mean:\n        best_model= model\n        best_model_name=f\"DistilBertSquad1_stateDict_ep{epoch+1}_DevF1{f1_mean:.3f}.dict\"\n        best_model_path = os.path.join(model_logfile_dir_path,f\"DistilBertSquad1_stateDict_ep{epoch+1}_DevF1{f1_mean:.3f}.dict\")\n        torch.save(best_model.state_dict(), best_model_path)\n        \n        max_f1_mean = f1_mean\n        ep = epoch\n    \n    # write to logfile\n    logfile.write(\"\\n\"+epoch_str+loss_str)\n    logfile.write(\"\\n\"+score_str+\"\\n\\n\")\n    \n    \n# torch.save(model.state_dict(), PATH)\nlogfile.close()\nnew_dir_name = f\"{model_logfile_dir_path}_ep{ep+1}__DevF1{f1_scores[ep]:.2f}\"\nos.rename(model_logfile_dir_path,new_dir_name)      \nmodel_logfile_dir_path=new_dir_name","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:53:03.338488Z","iopub.execute_input":"2022-07-28T07:53:03.338865Z","iopub.status.idle":"2022-07-28T08:40:00.383889Z","shell.execute_reply.started":"2022-07-28T07:53:03.338828Z","shell.execute_reply":"2022-07-28T08:40:00.382732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display files as links to download\nFileLinks(model_logfile_dir_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:50:57.780835Z","iopub.status.idle":"2022-07-28T07:50:57.781673Z","shell.execute_reply.started":"2022-07-28T07:50:57.781383Z","shell.execute_reply":"2022-07-28T07:50:57.781409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n---","metadata":{}},{"cell_type":"code","source":"best_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nbest_model_dict_path= os.path.join(model_logfile_dir_path, best_model_name)\n\nbest_model.load_state_dict(torch.load(best_model_dict_path))\n# best_model.load_state_dict(torch.load(f\"/kaggle/working/results/DistilBertSquad1_stateDict_ep2_DevF151.641.dict\"))\n\nbest_model.to(device)   \nm = best_model.eval()\n\ntotal_preds = total_preds_list(best_model,dev_dataset,device,dev_df)\nem,f1_mean,f1_max,score_str = evalEMandF1_squad1(dev_df,total_preds,model_logfile_dir_path)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:43:26.664363Z","iopub.execute_input":"2022-07-28T08:43:26.665318Z","iopub.status.idle":"2022-07-28T08:44:12.793611Z","shell.execute_reply.started":"2022-07-28T08:43:26.665280Z","shell.execute_reply":"2022-07-28T08:44:12.792293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions\n---","metadata":{}},{"cell_type":"code","source":"results_path = \"/kaggle/working/results/predictions\" #change it if not run in Kaggle environment\nif not os.path.exists(results_path):\n    os.makedirs(results_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T14:55:42.187228Z","iopub.execute_input":"2022-08-27T14:55:42.187623Z","iopub.status.idle":"2022-08-27T14:55:42.194047Z","shell.execute_reply.started":"2022-08-27T14:55:42.187588Z","shell.execute_reply":"2022-08-27T14:55:42.192546Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Question 2 <br> <span style=\"font-size:3.9mm;\">Manually insert questions</span>","metadata":{}},{"cell_type":"code","source":"# Load model\nbest_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nbest_model_dict_path= f\"/kaggle/input/bert-qna-256-squadv1/DistilBertSquad1_stateDict_ep2_DevF151.847.dict\"\nbest_model.load_state_dict(torch.load(best_model_dict_path))\nbest_model.to(device)   \nm = best_model.eval()\n\n# Load tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Handmade short context & questions evaluation\ntext = \"I am Theatina, I live in Athens and I believe life is an illusion !!\"\nprint(f\"\\nText: {text}\\n\")\n\n\nquestion = \"What's my name?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n\nquestion = \"What do I believe?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n\nquestion = \"Where does Theatina live?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-27T14:57:29.268020Z","iopub.execute_input":"2022-08-27T14:57:29.268459Z","iopub.status.idle":"2022-08-27T14:57:48.411885Z","shell.execute_reply.started":"2022-08-27T14:57:29.268421Z","shell.execute_reply":"2022-08-27T14:57:48.410768Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ccd5677f7745e2bb29c022edd1955e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7184fe78f11240d098498f5ebd57e616"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3b25b215e34e7fa9fbf9faab26f45f"}},"metadata":{}},{"name":"stdout","text":"\nText: I am Theatina, I live in Athens and I believe life is an illusion !!\n\nQuestion: What's my name?\nredicted answer: Theatina\n\nQuestion: What do I believe?\nPredicted answer: Life is illusion\n\nQuestion: Where does Theatina live?\nPredicted answer: Athens\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Question 3 <br> <span style=\"font-size:3.9mm;\">200 randomly chosen questions from training set</span>","metadata":{}},{"cell_type":"code","source":"# Load model\nbest_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nbest_model_dict_path= f\"/kaggle/input/bert-qna-256-squadv1/DistilBertSquad1_stateDict_ep2_DevF151.847.dict\"\nbest_model.load_state_dict(torch.load(best_model_dict_path))\nbest_model.to(device)   \nm = best_model.eval()\n\n# Load tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# load training set\nsquad_train_path = \"/kaggle/input/bert-code/bert/train-v1.1.json\"\ntrain_df = squad_df(squad_train_path)\nall_q_ids=train_df[\"id\"].values\n\n# randomly choose 200 questions\nquestions_ids = sample(set(all_q_ids),200)\ntrain_df_200 = train_df[train_df['id'].isin(questions_ids)].reset_index(drop=True)\ntrain_df_200[\"g_answer\"] = [ a[0][\"text\"] for a in train_df_200[\"answers\"]]\ncontext_list = train_df_200[\"context\"].values\nquestion_list = train_df_200[\"question\"].values\ng_answer_list = train_df_200[\"g_answer\"].values\n\n# answer questions and keep predictions for evaluation\ntotal_predictions_list=[]\nq_logfile_str=\"\"\nem,f1=\"Yes\",0.0\nfor num,(c,q,g_answer) in enumerate(zip(context_list,question_list,g_answer_list)):\n    answer=answer_a_question(q, text, tokenizer, best_model)\n    total_predictions_list+=answer\n    \n    f1 = compute_f1(g_answer, answer)\n    em_val = compute_exact_match(g_answer, answer)\n    if not em_val:\n        em=\"No\"\n    print(f\"\\nQ{num}: {question}\\nPredicted answer: {answer}\\nGolder answer: {g_answer}\\n(F1: {f1:.2f}%  |  EM: {em})\")\n    q_logfile_str+=f\"\\nQ{num}. {q}\\nA: {answer}\"\n    \n\nem,f1_mean,f1_max,score_str = evalEMandF1_squad1(train_df_200,total_predictions_list,results_path)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-27T16:09:23.751143Z","iopub.execute_input":"2022-08-27T16:09:23.751538Z","iopub.status.idle":"2022-08-27T16:09:36.415110Z","shell.execute_reply.started":"2022-08-27T16:09:23.751500Z","shell.execute_reply":"2022-08-27T16:09:36.413603Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nQ0: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: the Ku Klux Klan\n(F1: 0.00%  |  EM: No)\n\nQ1: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Book of Isaiah.\n(F1: 0.00%  |  EM: No)\n\nQ2: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: both in peace and wartime\n(F1: 0.00%  |  EM: No)\n\nQ3: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: penicillins and cephalosporins\n(F1: 0.00%  |  EM: No)\n\nQ4: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: develop resistance\n(F1: 0.00%  |  EM: No)\n\nQ5: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: October 1810\n(F1: 0.00%  |  EM: No)\n\nQ6: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep what is stated as having negative effect on frederics health\nGolder answer: bad weather\n(F1: 0.00%  |  EM: No)\n\nQ7: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Christoph Waltz\n(F1: 0.00%  |  EM: No)\n\nQ8: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: 10 minutes\n(F1: 0.00%  |  EM: No)\n\nQ9: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: All of the highways\n(F1: 0.00%  |  EM: No)\n\nQ10: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Ai Weiwei\n(F1: 0.00%  |  EM: No)\n\nQ11: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep what had to be evacuated due to potential flooding\nGolder answer: Entire villages\n(F1: 0.00%  |  EM: No)\n\nQ12: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 6\n(F1: 0.00%  |  EM: No)\n\nQ13: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 33%\n(F1: 0.00%  |  EM: No)\n\nQ14: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Vedas\n(F1: 0.00%  |  EM: No)\n\nQ15: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Karma\nGolder answer: rebirth\n(F1: 0.00%  |  EM: No)\n\nQ16: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Female dogs\nGolder answer: Spayed\n(F1: 0.00%  |  EM: No)\n\nQ17: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: over $180 billion\n(F1: 0.00%  |  EM: No)\n\nQ18: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Illusion sep most of forts\nGolder answer: ruins\n(F1: 0.00%  |  EM: No)\n\nQ19: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep what are some areas of learning in comprehensive schools\nGolder answer: design and technology and vocational learning\n(F1: 0.11%  |  EM: No)\n\nQ20: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Angolan\n(F1: 0.00%  |  EM: No)\n\nQ21: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Her Majesty's Most Honourable Privy Council\n(F1: 0.00%  |  EM: No)\n\nQ22: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Magistrate Judge Howard Lloyd\n(F1: 0.00%  |  EM: No)\n\nQ23: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: deployment of Canadian Defence Attachés\n(F1: 0.00%  |  EM: No)\n\nQ24: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: submarine service\n(F1: 0.00%  |  EM: No)\n\nQ25: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Entry lights\n(F1: 0.00%  |  EM: No)\n\nQ26: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Vasari\n(F1: 0.00%  |  EM: No)\n\nQ27: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Barbados\n(F1: 0.00%  |  EM: No)\n\nQ28: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Weightlifting\nGolder answer: soccer\n(F1: 0.00%  |  EM: No)\n\nQ29: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Oklahoma City National Memorial and Museum\n(F1: 0.00%  |  EM: No)\n\nQ30: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Cambridge, Massachusetts\n(F1: 0.00%  |  EM: No)\n\nQ31: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Falkland Islands and Ascension Island\n(F1: 0.00%  |  EM: No)\n\nQ32: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: reacting a flux of steam with metallic iron through an incandescent iron tube heated in a fire\n(F1: 0.00%  |  EM: No)\n\nQ33: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: New Jersey\n(F1: 0.00%  |  EM: No)\n\nQ34: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens and i believe life is illusion sep how many murders\nGolder answer: 39\n(F1: 0.00%  |  EM: No)\n\nQ35: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Boston\n(F1: 0.00%  |  EM: No)\n\nQ36: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Liquid telecom\nGolder answer: broadband\n(F1: 0.00%  |  EM: No)\n\nQ37: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Various interpretations\n(F1: 0.00%  |  EM: No)\n\nQ38: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: North and South Estonian languages\n(F1: 0.00%  |  EM: No)\n\nQ39: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Lines\nGolder answer: Laidlines\n(F1: 0.00%  |  EM: No)\n\nQ40: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: thousandths of an inch\n(F1: 0.00%  |  EM: No)\n\nQ41: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: more than two and a half centuries\n(F1: 0.00%  |  EM: No)\n\nQ42: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: How many hours\nGolder answer: 23\n(F1: 0.00%  |  EM: No)\n\nQ43: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: incumbents\n(F1: 0.00%  |  EM: No)\n\nQ44: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Countryside\nGolder answer: cities\n(F1: 0.00%  |  EM: No)\n\nQ45: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 15th\n(F1: 0.00%  |  EM: No)\n\nQ46: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Ferries\nGolder answer: Red Funnel\n(F1: 0.00%  |  EM: No)\n\nQ47: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Invalidate states consent\nGolder answer: used to obtain the consent of that state to a treaty\n(F1: 0.17%  |  EM: No)\n\nQ48: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Augustan\n(F1: 0.00%  |  EM: No)\n\nQ49: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Mali Empire\n(F1: 0.00%  |  EM: No)\n\nQ50: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Hengdian\n(F1: 0.00%  |  EM: No)\n\nQ51: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens and i believe life is illusion sep when was hauppauge 1212\nGolder answer: 2008\n(F1: 0.00%  |  EM: No)\n\nQ52: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: annual rings\n(F1: 0.00%  |  EM: No)\n\nQ53: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Good oak\nGolder answer: thick-walled\n(F1: 0.00%  |  EM: No)\n\nQ54: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: morphophonology\n(F1: 0.00%  |  EM: No)\n\nQ55: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Rwandan Genocide\n(F1: 0.00%  |  EM: No)\n\nQ56: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Continental intelligence\nGolder answer: official sources\n(F1: 0.00%  |  EM: No)\n\nQ57: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion\nGolder answer: German appeasement\n(F1: 0.00%  |  EM: No)\n\nQ58: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Great british bake off\nGolder answer: Nadiya Hussain\n(F1: 0.00%  |  EM: No)\n\nQ59: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: divine protection\n(F1: 0.00%  |  EM: No)\n\nQ60: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Illusion\nGolder answer: slower\n(F1: 0.00%  |  EM: No)\n\nQ61: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: into an invisible frame controlled by the attacker\n(F1: 0.00%  |  EM: No)\n\nQ62: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 20th century\n(F1: 0.00%  |  EM: No)\n\nQ63: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Proof Sets\n(F1: 0.00%  |  EM: No)\n\nQ64: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Francisco Franco\n(F1: 0.00%  |  EM: No)\n\nQ65: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Cædmon\n(F1: 0.00%  |  EM: No)\n\nQ66: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: airplane mode\n(F1: 0.00%  |  EM: No)\n\nQ67: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 1 March\n(F1: 0.00%  |  EM: No)\n\nQ68: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens and i believe life is illusion sep how many copies\nGolder answer: 8 million copies\n(F1: 0.10%  |  EM: No)\n\nQ69: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: November 22, 2006\n(F1: 0.00%  |  EM: No)\n\nQ70: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: July 4, 1776\n(F1: 0.00%  |  EM: No)\n\nQ71: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: a site for acid hydrolysis of microbial and dietary protein, preparing these protein sources for further digestion and absorption in the small intestine.\n(F1: 0.00%  |  EM: No)\n\nQ72: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life\nGolder answer: conflict of interest edits\n(F1: 0.00%  |  EM: No)\n\nQ73: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Adolescents or adults\nGolder answer: adolescents\n(F1: 0.50%  |  EM: No)\n\nQ74: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 1837\n(F1: 0.00%  |  EM: No)\n\nQ75: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Warmer or colder\nGolder answer: warmer\n(F1: 0.50%  |  EM: No)\n\nQ76: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: speed and direction\n(F1: 0.00%  |  EM: No)\n\nQ77: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: bhāṣā\n(F1: 0.00%  |  EM: No)\n\nQ78: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Many terms\n(F1: 0.00%  |  EM: No)\n\nQ79: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Estadi Ciutat de València\n(F1: 0.00%  |  EM: No)\n\nQ80: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Pittsfield, Massachusetts\n(F1: 0.00%  |  EM: No)\n\nQ81: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: 35,000\n(F1: 0.00%  |  EM: No)\n\nQ82: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: France\n(F1: 0.00%  |  EM: No)\n\nQ83: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: on an artillery gun carriage\n(F1: 0.00%  |  EM: No)\n\nQ84: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: the mitrailleuse\n(F1: 0.00%  |  EM: No)\n\nQ85: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: the attacking Prussian forces\n(F1: 0.00%  |  EM: No)\n\nQ86: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: glow-in-the-dark\n(F1: 0.00%  |  EM: No)\n\nQ87: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: bourgeois, egalitarian, rational, and independent from the state\n(F1: 0.00%  |  EM: No)\n\nQ88: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: philosophy\n(F1: 0.00%  |  EM: No)\n\nQ89: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 80,000 square feet\n(F1: 0.00%  |  EM: No)\n\nQ90: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Speaking to alexander\nGolder answer: fled\n(F1: 0.00%  |  EM: No)\n\nQ91: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: spoke English, nor read or wrote it\n(F1: 0.00%  |  EM: No)\n\nQ92: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Frederick Banting and his student Charles Best\n(F1: 0.00%  |  EM: No)\n\nQ93: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: opium\n(F1: 0.00%  |  EM: No)\n\nQ94: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep for what did canadian natives use asphalt as waterproofing\nGolder answer: canoes\n(F1: 0.00%  |  EM: No)\n\nQ95: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: 1861\n(F1: 0.00%  |  EM: No)\n\nQ96: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Nine pregnancies\nGolder answer: post-natal depression\n(F1: 0.00%  |  EM: No)\n\nQ97: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: an Italian refugee from Britain called Orsini\n(F1: 0.00%  |  EM: No)\n\nQ98: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep what was major reason victorias monarchy\nGolder answer: self-imposed isolation from the public\n(F1: 0.00%  |  EM: No)\n\nQ99: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Catholic rituals\nGolder answer: Public Worship Regulation Act 1874\n(F1: 0.00%  |  EM: No)\n\nQ100: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: 49 BCE\n(F1: 0.00%  |  EM: No)\n\nQ101: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Attalus I\n(F1: 0.00%  |  EM: No)\n\nQ102: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Charles\n(F1: 0.00%  |  EM: No)\n\nQ103: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Wildcats\n(F1: 0.00%  |  EM: No)\n\nQ104: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: vadam\n(F1: 0.00%  |  EM: No)\n\nQ105: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: What century\nGolder answer: 19th\n(F1: 0.00%  |  EM: No)\n\nQ106: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: The Guard Room\n(F1: 0.00%  |  EM: No)\n\nQ107: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: legitimize file-sharing\n(F1: 0.00%  |  EM: No)\n\nQ108: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Graecia\n(F1: 0.00%  |  EM: No)\n\nQ109: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: adipose tissue\n(F1: 0.00%  |  EM: No)\n\nQ110: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: U.S. Army's 1st, 2nd, 3rd and 4th\n(F1: 0.00%  |  EM: No)\n\nQ111: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Football\n(F1: 0.00%  |  EM: No)\n\nQ112: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion sep in 21st century\nGolder answer: the opening of new stores\n(F1: 0.00%  |  EM: No)\n\nQ113: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: 20 white and Hispanic firefighters\n(F1: 0.00%  |  EM: No)\n\nQ114: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Sport stadium\nGolder answer: New Haven Arena\n(F1: 0.00%  |  EM: No)\n\nQ115: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 1638\n(F1: 0.00%  |  EM: No)\n\nQ116: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: East coast greenway trail\nGolder answer: 3,000-mile\n(F1: 0.00%  |  EM: No)\n\nQ117: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Motion picture association of america\nGolder answer: film\n(F1: 0.00%  |  EM: No)\n\nQ118: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: The Presbyterian Church in Vanuatu\n(F1: 0.00%  |  EM: No)\n\nQ119: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Hong Kong\n(F1: 0.00%  |  EM: No)\n\nQ120: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Bay City, Michigan\n(F1: 0.00%  |  EM: No)\n\nQ121: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Madonna\n(F1: 0.00%  |  EM: No)\n\nQ122: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens and i believe life is illusion sep did all of political prisoners obtain freedom\nGolder answer: some other political prisoners have not been released\n(F1: 0.14%  |  EM: No)\n\nQ123: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Thein Sein became the first Myanmar president to visit the White House in 47 years\n(F1: 0.00%  |  EM: No)\n\nQ124: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: School\nGolder answer: Schooling is compulsory until the end of elementary school\n(F1: 0.22%  |  EM: No)\n\nQ125: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: outlet for their daily frustrations\n(F1: 0.00%  |  EM: No)\n\nQ126: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Factory\nGolder answer: A Royal Ordnance Factory, ROF Bridgwater was built at the start of the Second World War, between the villages of Puriton and Woolavington\n(F1: 0.10%  |  EM: No)\n\nQ127: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 1872 to 1909\n(F1: 0.00%  |  EM: No)\n\nQ128: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Notre Dame de Paris\n(F1: 0.00%  |  EM: No)\n\nQ129: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: dominion and conquest\n(F1: 0.00%  |  EM: No)\n\nQ130: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Large people living in poverty\nGolder answer: low levels of education\n(F1: 0.00%  |  EM: No)\n\nQ131: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: In ongoing border conflicts with the Frisians this first church was destroyed.\n(F1: 0.00%  |  EM: No)\n\nQ132: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Manhattan Project\n(F1: 0.00%  |  EM: No)\n\nQ133: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: 1348\n(F1: 0.00%  |  EM: No)\n\nQ134: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: PET scans and CAT scans\n(F1: 0.00%  |  EM: No)\n\nQ135: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Greek and Latin\n(F1: 0.00%  |  EM: No)\n\nQ136: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: an added prefix\n(F1: 0.00%  |  EM: No)\n\nQ137: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens and i believe life is illusion sep how many\nGolder answer: six series\n(F1: 0.00%  |  EM: No)\n\nQ138: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Six\nGolder answer: one\n(F1: 0.00%  |  EM: No)\n\nQ139: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Clubs in the next six levels (non-league football) are also eligible provided they have played in either the FA Cup, FA Trophy or FA Vase competitions\n(F1: 0.00%  |  EM: No)\n\nQ140: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Increase or decrease\nGolder answer: an increase\n(F1: 0.50%  |  EM: No)\n\nQ141: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I believe life is illusion sep how did darwin justify his theory not breaking down\nGolder answer: any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down.\n(F1: 0.16%  |  EM: No)\n\nQ142: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion\nGolder answer: Huxley had emphasised anatomical similarities between apes and humans, contesting Owen's view that humans were a separate sub-class\n(F1: 0.00%  |  EM: No)\n\nQ143: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Baikove Cemetery.\n(F1: 0.00%  |  EM: No)\n\nQ144: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: crux was also applied to objects other than a cross\n(F1: 0.00%  |  EM: No)\n\nQ145: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: philology\n(F1: 0.00%  |  EM: No)\n\nQ146: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: RPI Glee Club\n(F1: 0.00%  |  EM: No)\n\nQ147: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Procession of Boats\n(F1: 0.00%  |  EM: No)\n\nQ148: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: rhoticity\n(F1: 0.00%  |  EM: No)\n\nQ149: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 5.7 million\n(F1: 0.00%  |  EM: No)\n\nQ150: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Walls\nGolder answer: malachite\n(F1: 0.00%  |  EM: No)\n\nQ151: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life is illusion\nGolder answer: the inrush current\n(F1: 0.00%  |  EM: No)\n\nQ152: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Timothy McVeigh and Terry Nichols\n(F1: 0.00%  |  EM: No)\n\nQ153: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Oklahoma City\n(F1: 0.00%  |  EM: No)\n\nQ154: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Gonzalez paramo\nGolder answer: Luxembourg's Yves Mersch\n(F1: 0.00%  |  EM: No)\n\nQ155: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Sebastian\n(F1: 0.00%  |  EM: No)\n\nQ156: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: 56 – 7\nGolder answer: 2006\n(F1: 0.00%  |  EM: No)\n\nQ157: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: review of its legality\n(F1: 0.00%  |  EM: No)\n\nQ158: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Communication\nGolder answer: improve infrastructure\n(F1: 0.00%  |  EM: No)\n\nQ159: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Disco-Vision\n(F1: 0.00%  |  EM: No)\n\nQ160: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Kashmiri militants\n(F1: 0.00%  |  EM: No)\n\nQ161: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Tongass National Forest\n(F1: 0.00%  |  EM: No)\n\nQ162: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: Carl Gustav Hempel\n(F1: 0.00%  |  EM: No)\n\nQ163: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Samuel Siegel\n(F1: 0.00%  |  EM: No)\n\nQ164: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 12,884\n(F1: 0.00%  |  EM: No)\n\nQ165: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 1817\n(F1: 0.00%  |  EM: No)\n\nQ166: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Apollo\n(F1: 0.00%  |  EM: No)\n\nQ167: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Iwasaki Yatarō\n(F1: 0.00%  |  EM: No)\n\nQ168: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: video game\n(F1: 0.00%  |  EM: No)\n\nQ169: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: to prevent breakage\n(F1: 0.00%  |  EM: No)\n\nQ170: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Shifting taxes\nGolder answer: environmental tax reform\n(F1: 0.00%  |  EM: No)\n\nQ171: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Paracelsus\n(F1: 0.00%  |  EM: No)\n\nQ172: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: pair creation\n(F1: 0.00%  |  EM: No)\n\nQ173: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: northwestern Anatolia\n(F1: 0.00%  |  EM: No)\n\nQ174: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Mosques and bridges\nGolder answer: fountains and schools\n(F1: 0.33%  |  EM: No)\n\nQ175: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: accelerator mass spectrometry (AMS)\n(F1: 0.00%  |  EM: No)\n\nQ176: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: February 20, 1969\n(F1: 0.00%  |  EM: No)\n\nQ177: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Fatiha al nuri\nGolder answer: 1970\n(F1: 0.00%  |  EM: No)\n\nQ178: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Gold Hugo Lifetime Achievement Award\n(F1: 0.00%  |  EM: No)\n\nQ179: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: in the hoist-way\n(F1: 0.00%  |  EM: No)\n\nQ180: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Red\nGolder answer: subtractive primary colors\n(F1: 0.00%  |  EM: No)\n\nQ181: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 7 September 1940\n(F1: 0.00%  |  EM: No)\n\nQ182: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Abū Rayhān al-Bīrūnī\n(F1: 0.00%  |  EM: No)\n\nQ183: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Seismometer\nGolder answer: inverted\n(F1: 0.00%  |  EM: No)\n\nQ184: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt\n(F1: 0.00%  |  EM: No)\n\nQ185: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: 175 km2 (68 sq mi)\n(F1: 0.00%  |  EM: No)\n\nQ186: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: between a third of a chromosome up to the whole chromosome\n(F1: 0.00%  |  EM: No)\n\nQ187: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Rome\nGolder answer: two Etruscan towns\n(F1: 0.00%  |  EM: No)\n\nQ188: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: Masjid Muhammad #24\n(F1: 0.00%  |  EM: No)\n\nQ189: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Bronze Age\n(F1: 0.00%  |  EM: No)\n\nQ190: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina\nGolder answer: 1863\n(F1: 0.00%  |  EM: No)\n\nQ191: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: Accra in June of that year\n(F1: 0.00%  |  EM: No)\n\nQ192: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: bomber aircraft\n(F1: 0.00%  |  EM: No)\n\nQ193: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Athens\nGolder answer: University of Bologna\n(F1: 0.00%  |  EM: No)\n\nQ194: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: highest\n(F1: 0.00%  |  EM: No)\n\nQ195: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Theatina\nGolder answer: professor\n(F1: 0.00%  |  EM: No)\n\nQ196: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: \nGolder answer: neutral\n(F1: 0.00%  |  EM: No)\n\nQ197: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Not including hispanic\nGolder answer: 430,600\n(F1: 0.00%  |  EM: No)\n\nQ198: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: I am theatina i live in athens\nGolder answer: 1849\n(F1: 0.00%  |  EM: No)\n\nQ199: Who worshipped at Wanga Akash Bhairabh in ancient times?\nPredicted answer: Life\nGolder answer: trophy hunting\n(F1: 0.00%  |  EM: No)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/599640766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalEMandF1_squad1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_predictions_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_18/2933061138.py\u001b[0m in \u001b[0;36mevalEMandF1_squad1\u001b[0;34m(dev_df, total_preds, model_dir)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mlogfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Model Question Predictions on Dev Set\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mtrue_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gold_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid_to_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/2933061138.py\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(qid, total_preds)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"],"ename":"TypeError","evalue":"list indices must be integers or slices, not str","output_type":"error"}]},{"cell_type":"markdown","source":"## 📚 References\n\n[SQuAD: 100,000+ Questions for Machine Comprehension of Tex](https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf)<br>\n[Evaluating QA: Metrics, Predictions, and the Null Response](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/2020-06-09-Evaluating_BERT_on_SQuAD.ipynb)<br>\n[How-to Fine-Tune a Q&A Transformer](https://towardsdatascience.com/how-to-fine-tune-a-q-a-transformer-86f91ec92997)<br>\n[NLP - Document Retrieval for Question Answering](https://www.kaggle.com/code/leomauro/nlp-document-retrieval-for-question-answering)<br>\n[DistilBERT base model (uncased) ](https://huggingface.co/distilbert-base-uncased)<br>\n[SQuAD v1.1](https://datarepository.wolframcloud.com/resources/SQuAD-v1.1)<br>\n[kamalkraj/BERT-SQuAD ](https://github.com/kamalkraj/BERT-SQuAD)<br>\n[Question answering](https://huggingface.co/docs/transformers/tasks/question_answering)<br>\n\n\n\n\n","metadata":{}}]}