{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n\n# M915 - Συστήματα Κατανόησης και Παραγωγής Κειμένου <br> <span style=\"font-size:6mm;\"> Assignment 2 </span> <br><br> <span style=\"font-size:5mm;\"> Kylafi Christina-Theano </span> <br> <span style=\"font-size:4mm;\"> LT1200012 </span>\n---\n---","metadata":{}},{"cell_type":"markdown","source":"## Imports\n---","metadata":{}},{"cell_type":"code","source":"# import Python libraries\n\n# essentials\nimport os\nimport random\nfrom random import sample\nimport numpy as np\nfrom numpy import mean, std\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json\nfrom collections import Counter\nimport re, string, unicodedata\nimport pickle \nfrom datetime import datetime\nimport pytz\nfrom itertools import cycle\nfrom scipy import interp \nimport time\nimport copy\nimport json\nimport csv\nfrom ast import literal_eval\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\n# SKLEARN\nimport sklearn\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import normalize, OneHotEncoder, label_binarize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, log_loss, plot_confusion_matrix, roc_curve, auc, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import utils\nfrom sklearn.svm import SVC\n\n# NLTK\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nnltk.download\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n\n!pip install datasets\nfrom datasets import load_metric\n\n# BERT\n# !pip install transformers\n# !pip install pytorch-pretrained-bert\n\nimport transformers\nfrom transformers.data.processors.squad import SquadV2Processor\nfrom transformers.data import squad_convert_examples_to_features\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizer, BertModel, BertForPreTraining, BertTokenizerFast, AdamW, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering, DistilBertTokenizerFast, DistilBertModel\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\nimport warnings\n\n# MORE INSTALLATIONS & IMPORTS\n# !pip install yellowbrick\n# !pip install advertools\n# !pip install vaderSentiment\n# !pip install ekphrasis\n# !pip install tweet-preprocessor\n\nfrom wordcloud import WordCloud\n# import advertools as adv\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# from ekphrasis.classes.segmenter import Segmenter\n# import preprocessor as p\nimport multiprocessing\nfrom shutil import copy\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nfrom IPython.display import Image, FileLink, FileLinks, clear_output\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\n# clear_output()\n\nprint(\"\\nImports Done !\\n\")\n\n# Device settings\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Working on {device}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T12:17:59.280174Z","iopub.execute_input":"2022-08-29T12:17:59.281053Z","iopub.status.idle":"2022-08-29T12:18:08.981398Z","shell.execute_reply.started":"2022-08-29T12:17:59.281012Z","shell.execute_reply":"2022-08-29T12:18:08.980276Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.5.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\nImports Done !\n\nWorking on cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Functions\n---","metadata":{}},{"cell_type":"code","source":"# Functions\n\n# convert squad dataset to dataframe by extracting the necessary info\ndef squad_df(file_path, record_path=['data','paragraphs','qas','answers']):\n    file = json.loads(open(file_path).read())\n    \n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file,record_path[:-2])\n    \n    # combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    m['context'] = idx\n    data = m[['id','context','question','answers']].set_index('id').reset_index()\n    # keep the text sequence number for later\n    data['context_id'] = data['context'].factorize()[0]\n    return data\n\n\n# Add answer ending index\ndef add_ans_ind(qna_df):\n    texts = qna_df[\"context\"]\n    answers = qna_df[\"answers\"]\n    for row,(text,answers) in enumerate(zip(texts,answers)):\n        for a_num,a in enumerate(answers):\n            a_text = a[\"text\"]\n            a_start = a[\"answer_start\"]\n            a_end = a_start\n            \n            a_end = int(a_start+len(a_text))\n            if text[a_start:a_end] == a_text:\n                qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]=a_end\n            else:\n                if a_start==0:\n                    continue\n                else:\n                    if a_start==1:\n                        if text[a_start-1:a_end-1] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 1\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 1\n\n                    else:\n                        if text[a_start-2:a_end-2] == a_text:\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_start\"]= a_start - 2\n                            qna_df.loc[row,\"answers\"][a_num][\"answer_end\"]= a_end - 2\n\n    return qna_df\n\n\n# add the starting-ending positions of answers (token-wise)\ndef add_token_positions(encodings, texts, starts, ends):\n    # initialize lists to contain the token indices of answer start/end\n    start_pos = []\n    end_pos = []\n    unanswerable_pos = tokenizer.model_max_length\n    for i,(text,start,end) in enumerate( zip(texts,starts,ends) ):\n        # unanswerable questions\n        if start==end==len(text):\n            start_pos.append(unanswerable_pos)\n            end_pos.append(unanswerable_pos)\n            continue\n        else:\n            start_pos.append(encodings.char_to_token(i, start))\n            end_pos.append(encodings.char_to_token(i, end))\n            if start_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n                continue\n           \n            shift = 1\n            while end_pos[-1] is None and end-shift>start:\n                end_pos[-1] = encodings.char_to_token(i, end - shift)\n                shift += 1\n            if end_pos[-1] is None:\n                start_pos[-1] = unanswerable_pos\n                end_pos[-1] = unanswerable_pos\n\n    encodings.update({'start_positions': start_pos, 'end_positions': end_pos})\n\n# apply function to our data\n# add_token_positions(dev_encodings, list(qna_dev_df['AnswerStart']), list(qna_dev_df['AnswerEnd']))\n\n\ndef save_json_evalsquad1(total_preds,filepath=\"preds.json\", dev_set=\"/content/dev-v2.0.json\"):\n    with open(filepath, \"w\") as outfile:\n        json.dump(ast.literal_eval(json.dumps(total_preds)), outfile)\n    \n#     !python3 /content/evaluate-v2.0.py {dev_set} {filepath}\n    !python3 /kaggle/input/bert-code/bert/evaluate.py {dev_set} {filepath} \n\n    \ndef get_prediction(qid,total_preds):\n    return total_preds[qid]\n\ndef normalize_text(s):\n    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch if ch not in exclude else \" \" for ch in text)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef compute_exact_match(prediction, truth):\n    return int(normalize_text(prediction) == normalize_text(truth))\n\ndef compute_f1(prediction, truth):\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n    \n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n    \n    common_tokens = set(pred_tokens) & set(truth_tokens)\n    \n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n    \n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n    \n    return 2 * (prec * rec) / (prec + rec)\n\n\n\ndef get_gold_answers(qid,id_to_answers):\n    \n    gold_answers = [answer[\"text\"] for answer in id_to_answers[qid] if answer[\"text\"]]\n\n    # if gold_answers doesn't exist it's because this is a negative example - \n    # the only correct answer is an empty string\n    if not gold_answers:\n        gold_answers = [\"\"]\n    \n    return gold_answers\n\ndef evalEMandF1_squad1(dev_df,total_preds,model_dir):\n    global id_to_answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n    id_s = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n#     id_to_answers = {k:v for k,v in zip(id_s,answer_s)}\n    em_score = []\n    f1_score = []\n\n#   logfile for predictions and true answers\n    logfile_name = \"model_preds.txt\"\n    logfile_path = os.path.join( os.path.join(model_dir),logfile_name)\n    logfile = open(logfile_path, \"w\", encoding=\"utf-8\")\n    logfile.write(\"Best Model Question Predictions on Dev Set\\n\")\n    for num,(qid,quest) in enumerate(zip(id_s,questions)):\n        prediction = get_prediction(qid,total_preds)\n        true_answers = get_gold_answers(qid,id_to_answers)\n\n        em_score.append(max((compute_exact_match(prediction, answer)) for answer in true_answers))\n        f1_score.append(max((compute_f1(prediction, answer)) for answer in true_answers))\n        \n        log_str = f\"\\n\\n{num+1}. Question: {quest}\\nTrue Answer(s): {true_answers}\\nPrediction(s): {prediction}\\nEM: {bool(int(em_score[-1]))}\\nF1: {f1_score[-1]*100:.2f}%\"\n        logfile.write(log_str)\n    \n    \n    em = sum(em_score)/len(id_s)*100\n    f1_mean = sum(f1_score)/len(id_s)*100\n    f1_max = max(f1_score)*100\n    score_str = f\"\\nDev Set Scores -- \\tEM: {em:.2f}% \\tF1 (mean): {f1_mean:.2f}% \\tF1 (max): {f1_max:.2f}%\\n\"\n    print(score_str)\n    \n    logfile.write(\"\\n\\n\"+score_str)\n    logfile.close()\n    \n    return em,f1_mean,f1_max,score_str\n\n\ndef total_preds_list(model,dev_dataset,device,dev_df):\n    global id_to_answers\n    model.to(device)\n    model.eval()\n    # initialize list to store epoch accuracies\n\n    total_preds = {}\n    acc = []\n    gtruth = {}\n    predictions = {}\n\n    batch_size=64\n    val_loader = DataLoader(dev_dataset, batch_size=batch_size)\n    id_list = [ i for i in dev_df[\"id\"].values ]\n    batch_sizes= [ len(val_b[\"input_ids\"]) for val_b in val_loader]\n    batches=len(val_loader)\n    id_loader = [ id_list[i*batch_size:i*batch_size+batch_size] for i in range(batches-1) ]\n    id_loader.append( id_list[- batch_sizes[-1] :] )\n#     print(id_loader, batch_sizes, batches)\n\n    # create a dictionary to link question id's with answers\n    questions = [  q for q in dev_df[\"question\"].values  ]\n#     id_list = [ i for i in dev_df[\"id\"].values ]\n    answer_s = [ ans for ans in dev_df[\"answers\"].values ]\n    \n    loss_dev = 0\n    loop_dev = tqdm(val_loader,leave=True)\n    for batch,ids in zip(val_loader, id_loader):\n        # we don't need to calculate gradients as we're not training\n        with torch.no_grad():\n            # pull batched items from loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            # we will use true positions for accuracy calc\n    #         start_true = batch['start_positions'].to(device)\n    #         end_true = batch['end_positions'].to(device)\n            qid = ids\n            # make predictions\n            outputs = model(input_ids, attention_mask=attention_mask)\n#             loss = outputs[0]\n#             loss_dev += loss.item()\n    #         print(outputs['start_logits'])\n            # pull prediction tensors out and argmax to get predicted tokens\n            start_pred = torch.argmax(outputs['start_logits'], dim=1)\n            end_pred = torch.argmax(outputs['end_logits'], dim=1)\n\n    #         ans_pred_list = [ (question_id,p) for question_id,p in zip(qid,preds) ]\n            preds = [ tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[num][start_pred[num]:end_pred[num]])) for num in range(len(qid)) ]\n            total_preds.update({ q:pred for q,pred in zip(qid,preds) })\n            \n            loop_dev.update(1)\n\n    return total_preds\n\n\ndef log_model(logfile,datetime_info,model,model_name,loss_func,optimizer,batch_size,normalization,max_s_len,pretrained):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    # logstring+= f\"\\n\\n\\n\\n{separator}\\n{separator}\\n\\n\"\n    logstring+= f\"{datetime_info} - {model_name}\\n\\n\"\n    logstring+= f\"Features: {model.embedding.embedding_dim}\\n\"\n    logstring+= f\"Max Sentence length: {max_s_len}\\n\"\n    logstring+= f\"Pretrained Embeddings: {pretrained}\\n\"\n    logstring+= f\"Normalization: {normalization}\\n\"\n    # logstring+= f\"Epochs: {epochs}\\n\"\n    logstring+= f\"Batch size: {batch_size}\\n\"\n    logstring+= f\"Optimizer: {optimizer}\\n\"\n    logstring+= f\"Loss function: {loss_func}\"\n    if type(loss_func) == torch.nn.CrossEntropyLoss:\n        logstring+= f\", weight: {loss_func.weight}\\n\"\n    else :\n        logstring+= \"\\n\"\n        logstring+= f\"Layers: {model}\\n\\n\"\n        logstring+= f\"{separator}\\n\\n\"\n\n    logfile.write(logstring)\n\n\ndef log_score(logfile,epoch,model,score_str):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    logstring+= f\"\\n\\n\\n> Epoch:{epoch}\\n\"\n    logstring+= score_str\n\n\n    logfile.write(logstring)\n\n\n# Logfiles\ndef get_file_ptr(drive_path,model_name):\n    tz = pytz.timezone('Europe/Athens')\n    datetime_info = f\"{datetime.now(tz):%d%m%y_%H%M}\"\n\n    logfile_name = f\"{model_name}__{datetime_info}.txt\"\n    model_logfile_dir_path = os.path.join(drive_path, f\"Results/Logfiles/{logfile_name[:-4]}\") \n    if not os.path.exists(model_logfile_dir_path):\n        os.makedirs(model_logfile_dir_path)  \n\n    logfile_path = os.path.join( os.path.join(model_logfile_dir_path),logfile_name)\n    logfile = open(logfile_path, \"w\", encoding=\"utf-8\")\n\n    return logfile, datetime_info, model_logfile_dir_path, logfile_path\n\n\ndef log_model_bert(logfile,datetime_info,model_name,optimizer,batch_size,max_s_len,train_len,val_len):\n    separator = \"\".join([ \"_\" for i in range(50) ])\n    logstring = \"\"\n\n    # logstring+= f\"\\n\\n\\n\\n{separator}\\n{separator}\\n\\n\"\n    logstring+= f\"{datetime_info} - {model_name}\\n\\n\"\n    logstring+= f\"Max Sentence length: {max_s_len}\\n\"\n    logstring+= f\"Batch size: {batch_size}\\n\"\n    logstring+= f\"Optimizer: {optimizer}\\n\"\n    logstring+= f\"Training Data: {train_len}\\n\"\n    logstring+= f\"Validation Data: {val_len}\\n\"\n    #   logstring+= f\"Layers: {model}\\n\\n\"\n    logstring+= f\"{separator}\\n\\n\"\n\n    logfile.write(logstring)\n    \n  \ndef preprocess_function(examples, tokenizer):\n    questions = [ q.strip() for q in list(examples[\"question\"])]\n    inputs = tokenizer(\n        questions,\n        list(examples[\"context\"]),\n        max_length=tokenizer.model_max_length,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\")\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"].values\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        if type(answers[i])==type(str):\n            answer = eval(answers[i][0])\n        else:\n            answer = answers[i][0]\n        start_char = answer[\"answer_start\"]\n        end_char = start_char + len(answer[\"text\"])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ndef answer_a_question(question,text,tokenizer,model):\n    model.to(device)\n    model.eval()\n    \n    \n    model_in=tokenizer.encode_plus(text, question, max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length', return_tensors='pt')\n    in_ids = model_in[\"input_ids\"].to(device)\n    mask = model_in[\"attention_mask\"].to(device)\n    \n    outputs = model(in_ids, attention_mask=mask)\n    start_pred = torch.argmax(outputs['start_logits'], dim=1).item()\n    end_pred = torch.argmax(outputs['end_logits'], dim=1).item()\n    \n    text_ids= model_in[\"input_ids\"][0]\n    text_tokens=tokenizer.convert_ids_to_tokens(text_ids)\n\n    tokens = tokenizer.convert_ids_to_tokens(text_ids[start_pred:end_pred+1])\n    answer = tokenizer.convert_tokens_to_string(tokens)\n    answer = normalize_text(answer).capitalize()\n    \n#     print(f\"\\nText: {text}\\nQuestion: {question}\\n\\nPredicted answer: {answer}\\n\")\n\n    return answer\n\n\n# Data Visualization\ndef data_stats(df, data_type, N):\n    plt.clf()\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n    tokenizer.model_max_length=N\n    enc = tokenizer(list(df['context']), truncation=True, max_length=tokenizer.model_max_length )\n    lens=[ len(i_id) for i_id in enc[\"input_ids\"] ]\n    plt.figure(figsize=(18,12))\n    plt.grid(True)\n    plt.title(f\"{data_type} Set Cumulative Sum\")\n    plt.hist(lens)\n    plt.savefig(os.path.join(results_path,f\"{data_type}_max_lens_Csum_maxLen{N}.png\"),dpi=300)\n    plt.show()\n\n    return lens,enc\n\ndef bar_plotting_lens(lens, data_type, val_to_show, N):\n    plt.clf()\n    lens_df=pd.DataFrame(lens)\n    len_desc = [ len for len in lens_df[0].value_counts().index[:val_to_show].tolist() ]\n    len_freq = [ freq for freq in lens_df[0].value_counts().tolist( )[:val_to_show] ]\n    fig = plt.figure(figsize =(48, 6))\n    bar_plot = plt.bar(len_desc, len_freq)\n    max_freq_ind = len_freq.index(max(len_freq))\n    bar_plot[max_freq_ind].set_color('m')\n    plt.xticks(len_desc)\n    plt.xlabel('Length')\n    plt.ylabel('Num of Texts')\n    plt.title(f\"{data_type} set context length\")\n    plt.grid(True)\n    # show plot\n    plt.savefig(os.path.join(results_path,f\"{data_type}_max_lens_maxLen{N}.png\"),dpi=300)\n    plt.show()\n    return len_desc[max_freq_ind]\n\ndef perc_Csum(lens, data_type):\n    len_vals=[50,100,150,200,256,300,512, max(lens)+1]\n    print(f\"\\nPercentage of {data_type} set context under:\")\n    for L in len_vals:\n        # plot cumulative summary and percentage of samples with context under length L\n         print(f\"{L} -> {sum([i<L for i in lens])/len(lens)*100:.2f}%\")\n    \ndef data_vis_unanswerable(df, encodings):\n    df[\"start\"] = [ ans[0][\"answer_start\"] for ans in df[\"answers\"].values ]\n    df[\"start_token_id\"] = [ encodings.char_to_token(i, start) for  i, start in enumerate(df[\"start\"].values) ] \n    return df\n\n# pr_list = add_ans_ind(qna_train_df_quac)\n# pr_list = add_ans_ind(qna_dev_df_quac)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:52:37.775972Z","iopub.execute_input":"2022-08-29T15:52:37.776659Z","iopub.status.idle":"2022-08-29T15:52:37.958785Z","shell.execute_reply.started":"2022-08-29T15:52:37.776597Z","shell.execute_reply":"2022-08-29T15:52:37.957284Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"## SQuADv1.1 Dataset\n---\n\n#### Steps:\n1. Add data \n2. Search for \"bert-code\" (https://www.kaggle.com/datasets/rndmnub/bert-code)\n\nor\n\n1. Save \"train-v1.1.json\" & \"dev-v1.1.json\" \n2. Change the \"squad_train_path\" & \"squad_dev_path\" variables below","metadata":{}},{"cell_type":"code","source":"# directories' paths\nsquad_train_path = \"/kaggle/input/bert-code/bert/train-v1.1.json\"\nsquad_dev_path = \"/kaggle/input/bert-code/bert/dev-v1.1.json\"\ntrain_df = squad_df(squad_train_path)\ndev_df = squad_df(squad_dev_path)\n\n# decrease train dataset for quicker training process\n# train_df = train_df.sample(frac=1).reset_index(drop=True)\n# train_df = train_df[: int(train_df.shape[0]/2)]\n# print(train_df.shape)\n\n# eval_v1_path = \"/kaggle/input/bert-code/bert/evaluate.py\"\n# eval_v2_path = \"/kaggle/input/httpsgithubcomsomiltgbert/evaluate-v2.0.py\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-29T15:49:50.868931Z","iopub.execute_input":"2022-08-29T15:49:50.869316Z","iopub.status.idle":"2022-08-29T15:49:58.657208Z","shell.execute_reply.started":"2022-08-29T15:49:50.869285Z","shell.execute_reply":"2022-08-29T15:49:58.655853Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# results directory\nresults_path = \"/kaggle/working/results\" #change it if not run in Kaggle environment\nif not os.path.exists(results_path):\n    os.makedirs(results_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T12:20:21.067699Z","iopub.execute_input":"2022-08-29T12:20:21.068037Z","iopub.status.idle":"2022-08-29T12:20:21.074535Z","shell.execute_reply.started":"2022-08-29T12:20:21.068002Z","shell.execute_reply":"2022-08-29T12:20:21.073405Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization\n---","metadata":{}},{"cell_type":"code","source":"# plot counts of N_max biggest length values of context in the dataset \nlens_to_check=[50,100,150,200,256,300,512,792,3263]\nfor N_max in lens_to_check:\n    lens_to_show=50\n    \n    lens_train,enc_train = data_stats(train_df, \"Train\", N_max)\n    bar_plotting_lens(lens_train,\"Train\", lens_to_show, N_max)\n    data_vis_unanswerable(train_df, enc_train)\n    unanswerable = train_df['start_token_id'].isna().values \n    print(f\"\\nUnanswerable TRAIN questions for MaxLen {N_max}: { (sum(answerable)/len(lens_train))*100:.2f}\")\n\n    lens_dev, enc_dev = data_stats(dev_df, \"Dev\", N_max) \n    bar_plotting_lens(lens_dev,\"Dev\", lens_to_show, N_max)\n    data_vis_unanswerable(dev_df,enc_dev)\n    unanswerable = dev_df['start_token_id'].isna().values \n    print(f\"\\nUnanswerable DEV questions for MaxLen {N_max}: { (sum(unanswerable)/len(lens_dev))*100:.2f}\")\n\n    \n# plot cumulative sum and percentage of samples with context under length L\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\nenc = tokenizer(list(train_df['context']), truncation=False)\nlens_train=[ len(i_id) for i_id in enc[\"input_ids\"] ]\nperc_Csum(lens_train, \"Train\")\n\nenc = tokenizer(list(dev_df['context']), truncation=False)\nlens_dev=[ len(i_id) for i_id in enc[\"input_ids\"] ]\nperc_Csum(lens_dev, \"Dev\")\n\nFileLinks(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lens = data_stats(train_df, \"Train\")\nsizes = [50,100,150,200,256,300,512, max(lens)+1]\nfor s in sizes:\n    print(f\"Max Len:{s}\\nUnanswerable Qs: {check_unanswerable(train_df, s):.2f}% \\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-27T18:26:19.121444Z","iopub.execute_input":"2022-08-27T18:26:19.122080Z","iopub.status.idle":"2022-08-27T18:27:03.068440Z","shell.execute_reply.started":"2022-08-27T18:26:19.122042Z","shell.execute_reply":"2022-08-27T18:27:03.067328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for pos in range(100):\n    print(len(train_df[\"context\"][pos]),train_df[\"answers\"][pos][0][\"answer_start\"], train_df[\"answers\"][pos][0][\"answer_start\"]+len(train_df[\"answers\"][0][0][\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2022-08-27T18:12:00.403550Z","iopub.execute_input":"2022-08-27T18:12:00.404231Z","iopub.status.idle":"2022-08-27T18:12:00.416628Z","shell.execute_reply.started":"2022-08-27T18:12:00.404194Z","shell.execute_reply":"2022-08-27T18:12:00.415522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing\n---","metadata":{}},{"cell_type":"code","source":"# clean GPU cache\nimport gc\nimport torch\ndef clean_GPU_cache(print_sum=False):\n    if print_sum:  \n        print(\"\\nBefore\\n\")\n        print(torch.cuda.memory_summary(device=device, abbreviated=False))\n    torch.cuda.memory_summary(device=None, abbreviated=False)\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.empty_cache()\n    if print_sum:  \n        print(\"\\n\\nAfter\\n\")\n        print(torch.cuda.memory_summary(device=device, abbreviated=False))\n\ndef check_gpu():\n    clean_GPU_cache()\n    print(\"\\n\")\n    !nvidia-smi\n    print(\"\\n\")\n\n# clean_GPU_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:52:53.644508Z","iopub.execute_input":"2022-08-29T15:52:53.645151Z","iopub.status.idle":"2022-08-29T15:52:53.653656Z","shell.execute_reply.started":"2022-08-29T15:52:53.645117Z","shell.execute_reply":"2022-08-29T15:52:53.652680Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# load distilBERT tokenizer & model\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer.model_max_length = 256\n\n# tokenize - truncate or pad only context if needed \ntrain_encodings = tokenizer(list(train_df['context']), list(train_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\ndev_encodings = tokenizer(list(dev_df['context']), list(dev_df['question']), max_length=tokenizer.model_max_length, truncation=\"only_first\", padding='max_length')\n\n# create the train/test encodings\n# reference: https://huggingface.co/docs/transformers/tasks/question_answering\n# train_encodings = preprocess_function(train_df, tokenizer)\n# dev_encodings = preprocess_function(dev_df, tokenizer)\n\n# id to answers dictionary\nglobal id_to_answers\nid_to_answers = { id_num:ans for id_num,ans in zip( dev_df[\"id\"].values, dev_df[\"answers\"].values ) }","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:49:58.660119Z","iopub.execute_input":"2022-08-29T15:49:58.660859Z","iopub.status.idle":"2022-08-29T15:50:52.878496Z","shell.execute_reply.started":"2022-08-29T15:49:58.660818Z","shell.execute_reply":"2022-08-29T15:50:52.877490Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# data processing\n# add end offset to the answers dictionaries\ntrain_df=add_ans_ind(train_df)\ndev_df=add_ans_ind(dev_df)\n\nanswer_starts_train=[ ans[0][\"answer_start\"] for ans in train_df[\"answers\"].values ]\nanswer_ends_train=[ ans[0][\"answer_end\"] for ans in train_df[\"answers\"].values ]\ntrain_encodings.update({\"start\":answer_starts_train, \"end\":answer_ends_train})\n\nanswer_starts=[ ans[0][\"answer_start\"] for ans in dev_df[\"answers\"].values ]\nanswer_ends=[ ans[0][\"answer_end\"] for ans in dev_df[\"answers\"].values ]\ndev_encodings.update({\"start\":answer_starts, \"end\":answer_ends})\n\nadd_token_positions(train_encodings, train_df[\"context\"], answer_starts_train, answer_ends_train)\n\n# create dataaframes with useful information\ntrain_enc_df=pd.DataFrame({ k:train_encodings[k] for k in train_encodings.keys() })\ndev_enc_df=pd.DataFrame({ k:dev_encodings[k] for k in dev_encodings.keys() })\n\n# save encodings\n# train_df.to_csv(os.path.join(results_path,f\"SQuADv1_train_encodings_{tokenizer.model_max_length}.csv\"), index=False, header=True)\n# dev_df.to_csv(os.path.join(results_path,f\"SQuADv1_dev_encodings_{tokenizer.model_max_length}.csv\"), index=False, header=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:50:52.880373Z","iopub.execute_input":"2022-08-29T15:50:52.880808Z","iopub.status.idle":"2022-08-29T15:50:54.705911Z","shell.execute_reply.started":"2022-08-29T15:50:52.880768Z","shell.execute_reply":"2022-08-29T15:50:54.704921Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# load encodings df\n# train_enc_path=f\"/kaggle/input/squadv1-encodings-bert{tokenizer.model_max_length}/SQuADv1_train_encodings_{tokenizer.model_max_length}.csv\"\n# train_encodings=pd.read_csv(train_enc_path)\n# dev_enc_path=f\"/kaggle/input/squadv1-encodings-bert{tokenizer.model_max_length}/SQuADv1_dev_encodings_{tokenizer.model_max_length}.csv\"\n# dev_encodings=pd.read_csv(dev_enc_path)\n\n# # keep half the data due to computational complexity\n# train_encodings=train_encodings[:int(train_encodings.shape[0]/4)]\n# print(train_encodings.shape[0])\n\n# # converse to dictionaries\n# train_encodings={ k:train_encodings[k].values for k in train_encodings.columns  }\n# dev_encodings={ k:dev_encodings[k].values for k in dev_encodings.columns  }","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:53:24.348209Z","iopub.execute_input":"2022-07-27T16:53:24.348589Z","iopub.status.idle":"2022-07-27T16:53:25.627115Z","shell.execute_reply.started":"2022-07-27T16:53:24.348558Z","shell.execute_reply":"2022-07-27T16:53:25.626115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTinput(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, ind):\n        return {key: torch.tensor(value[ind]) for key,value in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:50:54.707510Z","iopub.execute_input":"2022-08-29T15:50:54.707934Z","iopub.status.idle":"2022-08-29T15:50:54.715182Z","shell.execute_reply.started":"2022-08-29T15:50:54.707898Z","shell.execute_reply":"2022-08-29T15:50:54.713923Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nclean_GPU_cache()\nbatch_size=32\n\n# build datasets for both our training and validation sets\ntrain_dataset = BERTinput(train_encodings)\ndev_dataset = BERTinput(dev_encodings)\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# initialize validation set data loader\nval_loader = DataLoader(dev_dataset, batch_size=batch_size)\n\n# with open(f'/kaggle/..../train_loader_{tokenizer.model_max_length}.pickle', 'wb') as handle:\n#     pickle.dump(train_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:52:56.368840Z","iopub.execute_input":"2022-08-29T15:52:56.369210Z","iopub.status.idle":"2022-08-29T15:52:58.175206Z","shell.execute_reply.started":"2022-08-29T15:52:56.369180Z","shell.execute_reply":"2022-08-29T15:52:58.173782Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"## Training\n---","metadata":{}},{"cell_type":"code","source":"# model training\n\n# setup GPU/CPU\ncheck_gpu()\n\npretrained = 0\nif pretrained==0:\n    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n    model.to(device)    \n    model_name = \"DistilBertSQuADv1\"\nelse:\n    model_name = \"DistilBertSQuADv1Pretrained\"\n\n\nmodel.train()\ntot_eps = 5\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\nlr = 5e-5\n# optim = torch.optim.Adam(model.parameters(), lr=9e-6)\nweight_decay = 0.999\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\nlogfile, datetime_info, model_logfile_dir_path, logfilefullpath = get_file_ptr(results_path,model_name)\nlog_model_bert(logfile,datetime_info,model_name,optimizer,batch_size,tokenizer.model_max_length,len(train_dataset),len(dev_dataset))\n\noverall_train_loss = []\noverall_dev_loss = []\nmax_f1_mean = -99\nem_scores = []\nf1_scores = []\nep = 0\nloss_dev = []\nfor epoch in range(tot_eps):\n    epoch_loss_train = 0\n    epoch_acc = 0\n    epoch_acc_dev = 0\n    epoch_loss_dev = 0\n    # set model to train mode\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        \n        # initialize calculated gradients (from prev step)\n        optimizer.zero_grad()\n        \n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        \n        # train model on batch and return outputs (including loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        \n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch+1}')\n        loop.set_postfix(loss=loss.item())\n        loop.update(1)\n        \n        epoch_loss_train += loss.item()\n        \n    \n    ep_loss_train = epoch_loss_train / len(train_loader)\n    ep_loss_dev = epoch_loss_dev / len(val_loader)\n\n    # keeping loss for learning curve plots\n    overall_train_loss.append(ep_loss_train)\n\n    #     evaluation and EM & F1 scores\n    epoch_str = f'\\n----------------------- Epoch {epoch+1} / {tot_eps} -----------------------\\n' \n    print(epoch_str)\n    loss_str = f'Train Loss: {ep_loss_train:.3f}\\n'\n    print(loss_str)  \n    \n    tot_preds = total_preds_list(model,dev_dataset,device,dev_df)\n    em,f1_mean,f1_max,score_str = evalEMandF1_squad1(dev_df,tot_preds,model_logfile_dir_path)\n    em_scores.append(em)\n    f1_scores.append(f1_mean)\n    \n        # save best model\n    if f1_mean > max_f1_mean:\n        best_model= model\n        best_model_name=f\"DistilBertSquad1_stateDict_ep{epoch+1}_DevF1{f1_mean:.3f}.dict\"\n        best_model_path = os.path.join(model_logfile_dir_path,f\"DistilBertSquad1_stateDict_ep{epoch+1}_DevF1{f1_mean:.3f}.dict\")\n        torch.save(best_model.state_dict(), best_model_path)\n        \n        max_f1_mean = f1_mean\n        ep = epoch\n    \n    # write to logfile\n    logfile.write(\"\\n\"+epoch_str+loss_str)\n    logfile.write(\"\\n\"+score_str+\"\\n\\n\")\n    \n    \n# torch.save(model.state_dict(), PATH)\nlogfile.close()\nnew_dir_name = f\"{model_logfile_dir_path}_ep{ep+1}__DevF1{f1_scores[ep]:.2f}\"\nos.rename(model_logfile_dir_path,new_dir_name)      \nmodel_logfile_dir_path=new_dir_name","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:53:03.338488Z","iopub.execute_input":"2022-07-28T07:53:03.338865Z","iopub.status.idle":"2022-07-28T08:40:00.383889Z","shell.execute_reply.started":"2022-07-28T07:53:03.338828Z","shell.execute_reply":"2022-07-28T08:40:00.382732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display files as links to download\nFileLinks(model_logfile_dir_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T07:50:57.780835Z","iopub.status.idle":"2022-07-28T07:50:57.781673Z","shell.execute_reply.started":"2022-07-28T07:50:57.781383Z","shell.execute_reply":"2022-07-28T07:50:57.781409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n---","metadata":{}},{"cell_type":"code","source":"best_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n# best_model_dict_path= os.path.join(model_logfile_dir_path, best_model_name)\n\n# best_model.load_state_dict(torch.load(best_model_dict_path))\nbest_model.load_state_dict(torch.load(f\"/kaggle/input/bert-qna-256-squadv1/DistilBertSquad1_stateDict_ep2_DevF151.847.dict\"))\n\nbest_model.to(device)   \nm = best_model.eval()\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer.model_max_length=model_max_len\n\ntotal_preds = total_preds_list(best_model,dev_dataset,device,dev_df)\nem,f1_mean,f1_max,score_str = evalEMandF1_squad1(dev_df,total_preds,results_path)\n\nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:57:51.210671Z","iopub.execute_input":"2022-08-29T15:57:51.211302Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n 45%|████▌     | 75/166 [00:17<00:21,  4.15it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predictions\n---","metadata":{}},{"cell_type":"code","source":"results_path = \"/kaggle/working/results/predictions\" #change it if not run in Kaggle environment\nif not os.path.exists(results_path):\n    os.makedirs(results_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T14:55:42.187228Z","iopub.execute_input":"2022-08-27T14:55:42.187623Z","iopub.status.idle":"2022-08-27T14:55:42.194047Z","shell.execute_reply.started":"2022-08-27T14:55:42.187588Z","shell.execute_reply":"2022-08-27T14:55:42.192546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question 2 <br> <span style=\"font-size:3.9mm;\">Manually insert questions</span>","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel_max_len=256\nbest_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nbest_model_dict_path= f\"/kaggle/input/bert-qna-{model_max_len}-squadv1/DistilBertSquad1_stateDict_ep2_DevF151.847.dict\"\nbest_model.load_state_dict(torch.load(best_model_dict_path))\nbest_model.to(device)   \nm = best_model.eval()\n\n# Load tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer.model_max_length=model_max_len\n\n# Handmade short context & questions evaluation\ntext = \"I am Theatina, I live in Athens and I believe life is an illusion !!\"\nprint(f\"\\nText: {text}\\n\")\n\nquestion = \"What's my name?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n\nquestion = \"What do I believe?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n\nquestion = \"Where does Theatina live?\"\nanswer=answer_a_question(question, text, tokenizer, best_model)\nprint(f\"Question: {question}\\nPredicted answer: {answer}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-27T17:30:51.013657Z","iopub.execute_input":"2022-08-27T17:30:51.014125Z","iopub.status.idle":"2022-08-27T17:30:56.392986Z","shell.execute_reply.started":"2022-08-27T17:30:51.014091Z","shell.execute_reply":"2022-08-27T17:30:56.391012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question 3 <br> <span style=\"font-size:3.9mm;\">200 randomly chosen questions from training set</span>","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel_max_len=256\nbest_model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nbest_model_dict_path= f\"/kaggle/input/bert-qna-{model_max_len}-squadv1/DistilBertSquad1_stateDict_ep2_DevF151.847.dict\"\nbest_model.load_state_dict(torch.load(best_model_dict_path))\nbest_model.to(device)   \nm = best_model.eval()\n\n# Load tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer.model_max_length=model_max_len\n\n# load set\n# set_for_evaluation=\"train\"\nset_for_evaluation=\"dev\"\nsquad_path = f\"/kaggle/input/bert-code/bert/{set_for_evaluation}-v1.1.json\"\neval_df = squad_df(squad_path)\nall_q_ids=eval_df[\"id\"].values\n\n# randomly choose 200 questions\nN=200\nquestions_ids = sample(set(all_q_ids),N)\neval_df_200 = eval_df[eval_df['id'].isin(questions_ids)].reset_index(drop=True)\neval_df_200[\"g_answer\"] = [ a[0][\"text\"] for a in eval_df_200[\"answers\"]]\ncontext_list = eval_df_200[\"context\"].values\nquestion_list = eval_df_200[\"question\"].values\ng_answer_list = eval_df_200[\"g_answer\"].values\n\n# answer questions and keep predictions for evaluation\ntotal_predictions_list=[]\nq_logfile_str=\"\"\nem,f1=\"Yes\",0.0\nmean_em, mean_f1=0.0,0.0\nfor num,(c,q,g_answer) in enumerate(zip(context_list,question_list,g_answer_list)):\n    answer=answer_a_question(q, c, tokenizer, best_model)\n    total_predictions_list+=answer\n    \n    f1 = compute_f1(g_answer, answer)\n    mean_f1+=f1\n        \n    em_val = compute_exact_match(g_answer, answer)\n    mean_em+=em_val\n    \n    em = \"Yes\" if em_val else \"No\"\n    q_logfile_str+=f\"\\nQ{num+1}. {q}\\nPredicted answer: {answer}\\nGolder answer: {g_answer}\\n(F1: {f1*100:.2f}%  |  EM: {em})\\n\"\n    \nmean_f1=mean_f1/N\nmean_em=mean_em/N\nscore_str=f\"\\nMean F1: {mean_f1*100:.2f}%  |  Mean EM: {mean_em*100:.2f}%\\n\"\n\nlogfile=os.path.join(results_path, f\"{set_for_evaluation.capitalize()}_200_questions.txt\")    \nwith open(logfile, \"w\", encoding=\"utf-8\") as writer:\n    writer.write(score_str+\"\\n\")\n    writer.write(q_logfile_str)\n    \nq_logfile_str_out=score_str+q_logfile_str\nprint(q_logfile_str_out)\n    \nFileLinks(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:07:47.355134Z","iopub.execute_input":"2022-08-29T15:07:47.355512Z","iopub.status.idle":"2022-08-29T15:08:00.476723Z","shell.execute_reply.started":"2022-08-29T15:07:47.355481Z","shell.execute_reply":"2022-08-29T15:08:00.475657Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nMean F1: 73.64%  |  Mean EM: 61.00%\n\nQ1. Super Bowl 50 determined the NFL champion for what season?\nPredicted answer: 2015\nGolder answer: 2015\n(F1: 100.00%  |  EM: Yes)\n\nQ2. Who was the main performer at this year's halftime show?\nPredicted answer: Beyonce and bruno mars\nGolder answer: Coldplay\n(F1: 0.00%  |  EM: No)\n\nQ3. How much did it cost to build Levi's Stadium?\nPredicted answer: 1 2 billion\nGolder answer: $1.2 billion\n(F1: 100.00%  |  EM: Yes)\n\nQ4. What was the final score of the game between the Panthers and the Seahawks?\nPredicted answer: 49 – 15\nGolder answer: 31–24\n(F1: 0.00%  |  EM: No)\n\nQ5. A replica of what landmark was present at Super Bowl Opening Night?\nPredicted answer: Golden gate bridge\nGolder answer: the Golden Gate Bridge\n(F1: 100.00%  |  EM: Yes)\n\nQ6. What percentage of funds were given as charitable funds to causes in and around San Francisco?\nPredicted answer: 25\nGolder answer: 25 percent\n(F1: 66.67%  |  EM: No)\n\nQ7. On what service could the ESPN Deportes broadcast be streamed?\nPredicted answer: Watchespn\nGolder answer: WatchESPN\n(F1: 100.00%  |  EM: Yes)\n\nQ8. Along with Dan Fouts, who served as a color analyst for the radio broadcast?\nPredicted answer: Boomer esiason\nGolder answer: Boomer Esiason\n(F1: 100.00%  |  EM: Yes)\n\nQ9. What actor did sign language for the National Anthem at Superbowl 50?\nPredicted answer: Marlee matlin\nGolder answer: Marlee Matlin\n(F1: 100.00%  |  EM: Yes)\n\nQ10. Who recovered Ward's fumble?\nPredicted answer: Safety t j ward ward fumbled ball during return but trevathan\nGolder answer: Trevathan\n(F1: 16.67%  |  EM: No)\n\nQ11. How many solo sacks did Von Miller have in the game?\nPredicted answer: Five\nGolder answer: five\n(F1: 100.00%  |  EM: Yes)\n\nQ12. How many intercpetions did Newton have in Super Bowl 50?\nPredicted answer: 18\nGolder answer: one\n(F1: 0.00%  |  EM: No)\n\nQ13. What team had 244 yards in Super Bowl XXXV?\nPredicted answer: Baltimore ravens\nGolder answer: Baltimore Ravens\n(F1: 100.00%  |  EM: Yes)\n\nQ14. What is the name of the European Union agency for external border security?\nPredicted answer: Frontex\nGolder answer: Frontex\n(F1: 100.00%  |  EM: Yes)\n\nQ15. Why did Warsaw become the capital of the Commonwealth?\nPredicted answer: Due to its central location\nGolder answer: Due to its central location\n(F1: 100.00%  |  EM: Yes)\n\nQ16. What is the axis of Warsaw which divides it into two parts?\nPredicted answer: Vistula river\nGolder answer: Vistula River\n(F1: 100.00%  |  EM: Yes)\n\nQ17. What park is close to John Lennon street?\nPredicted answer: Park ujazdowski\nGolder answer: Park Ujazdowski\n(F1: 100.00%  |  EM: Yes)\n\nQ18. When did the Warsaw area enlargement take place?\nPredicted answer: 1951\nGolder answer: 1951\n(F1: 100.00%  |  EM: Yes)\n\nQ19. What part of France were the Normans located?\nPredicted answer: North\nGolder answer: north\n(F1: 100.00%  |  EM: Yes)\n\nQ20. Who bought the rights?\nPredicted answer: Enrique perez de guzman 2nd count de niebla\nGolder answer: Enrique Pérez de Guzmán\n(F1: 33.33%  |  EM: No)\n\nQ21. When did Tesla go to Karlovac?\nPredicted answer: 1870\nGolder answer: 1870\n(F1: 100.00%  |  EM: Yes)\n\nQ22. What was one of theories as to what caused Tesla's father's unspecified illness?\nPredicted answer: Stroke\nGolder answer: stroke\n(F1: 100.00%  |  EM: Yes)\n\nQ23. Who was Alfred S brown?\nPredicted answer: Charles f peck\nGolder answer: a Western Union superintendent\n(F1: 0.00%  |  EM: No)\n\nQ24. To where were the belongings taken?\nPredicted answer: Manhattan storage and warehouse company\nGolder answer: Manhattan Storage and Warehouse Company\n(F1: 100.00%  |  EM: Yes)\n\nQ25. How old was Tesla when he wrote that he'd completed his dynamic theory of gravity?\nPredicted answer: 81\nGolder answer: 81\n(F1: 100.00%  |  EM: Yes)\n\nQ26. What kind of fiction is Tesla's work featured in?\nPredicted answer: Science fiction\nGolder answer: science fiction\n(F1: 100.00%  |  EM: Yes)\n\nQ27. What is the expression used to denote a worst case complexity as expressed by time taken?\nPredicted answer: O n2\nGolder answer: O(n2)\n(F1: 100.00%  |  EM: Yes)\n\nQ28. Who demonstrated that P= NP implies problems not present in P or NP-complete?\nPredicted answer: Ladner\nGolder answer: Ladner\n(F1: 100.00%  |  EM: Yes)\n\nQ29. What variable is associated with all problems solved within logarithmic space?\nPredicted answer: L\nGolder answer: L\n(F1: 100.00%  |  EM: Yes)\n\nQ30. How common of a type was corporal punishment in schools?\nPredicted answer: Most\nGolder answer: the most common\n(F1: 66.67%  |  EM: No)\n\nQ31. When was the Teaching Council Act passed?\nPredicted answer: 2001\nGolder answer: 2001\n(F1: 100.00%  |  EM: Yes)\n\nQ32. Who cannot be employed by a school in any manner?\nPredicted answer: Those who refuse vetting\nGolder answer: those who refuse vetting\n(F1: 100.00%  |  EM: Yes)\n\nQ33. What type of positions would these counties be trying to recruit for?\nPredicted answer: Hard to fill\nGolder answer: hard-to-fill positions\n(F1: 85.71%  |  EM: No)\n\nQ34. What two factors can generally increase a teacher's salary?\nPredicted answer: \nGolder answer: more experience and higher education\n(F1: 0.00%  |  EM: No)\n\nQ35. Which denomination has more of an individualistic streak?\nPredicted answer: Catholic\nGolder answer: Protestant\n(F1: 0.00%  |  EM: No)\n\nQ36. Into what religion was Martin Luther baptized?\nPredicted answer: Catholic\nGolder answer: Catholic\n(F1: 100.00%  |  EM: Yes)\n\nQ37. When was Martin Luther ordained as a priest?\nPredicted answer: 1507\nGolder answer: 1507\n(F1: 100.00%  |  EM: Yes)\n\nQ38. What degree did Martin Luther receive on 19 October, 1512?\nPredicted answer: Doctor of theology\nGolder answer: Doctor of Theology\n(F1: 100.00%  |  EM: Yes)\n\nQ39. Who did Luther say that Christians must not slacken in following?\nPredicted answer: Christ\nGolder answer: Christ\n(F1: 100.00%  |  EM: Yes)\n\nQ40. What was the first point of the Reformation?\nPredicted answer: Christ and his salvation\nGolder answer: Christ and His salvation\n(F1: 100.00%  |  EM: Yes)\n\nQ41. After Archbishop Albrecht reviewed the Theses, where did he send them?\nPredicted answer: Rome\nGolder answer: Rome\n(F1: 100.00%  |  EM: Yes)\n\nQ42. When did Archbishop Albrecht send Luther's letter containing the 95 Theses to Rome?\nPredicted answer: December 1517\nGolder answer: December 1517\n(F1: 100.00%  |  EM: Yes)\n\nQ43. What was the title of the first choral hymnal?\nPredicted answer: 32 songs\nGolder answer: Eyn geystlich Gesangk Buchleyn\n(F1: 0.00%  |  EM: No)\n\nQ44. What did Luther expound happened to souls after death?\nPredicted answer: Penitential suffering\nGolder answer: sleep in peace\n(F1: 0.00%  |  EM: No)\n\nQ45. What did Agricola apparently believe about who should be in control  law?\nPredicted answer: City hall\nGolder answer: city hall\n(F1: 100.00%  |  EM: Yes)\n\nQ46. What does refusing to preach the Ten Commandments not do?\nPredicted answer: Eliminate accusing law\nGolder answer: eliminate the accusing law\n(F1: 100.00%  |  EM: Yes)\n\nQ47. What event took away his ability of speech?\nPredicted answer: Apoplectic stroke\nGolder answer: apoplectic stroke\n(F1: 100.00%  |  EM: Yes)\n\nQ48. What famous snowbaorder lives in southern California?\nPredicted answer: Shaun white\nGolder answer: Shaun White\n(F1: 100.00%  |  EM: Yes)\n\nQ49. Southern California is second to which island in terms of famous serf breaks?\nPredicted answer: Oahu\nGolder answer: Oahu\n(F1: 100.00%  |  EM: Yes)\n\nQ50. Other than for its resort feel, what is Palm Springs popular for?\nPredicted answer: Desert city of palm springs is popular for its resort feel and nearby open spaces\nGolder answer: open spaces\n(F1: 23.53%  |  EM: No)\n\nQ51. There are 34 cities in southern California that have a population exceeding what number?\nPredicted answer: Over 100 000\nGolder answer: 100,000\n(F1: 80.00%  |  EM: No)\n\nQ52. Southern California is most famous for tourism and what notably named district?\nPredicted answer: Hollywood\nGolder answer: Hollywood\n(F1: 100.00%  |  EM: Yes)\n\nQ53. Which county is developing its business center?\nPredicted answer: Orange county\nGolder answer: Orange\n(F1: 66.67%  |  EM: No)\n\nQ54. What is the other NHL team aside from the Anaheim Ducks to reside in Southern California?\nPredicted answer: Los angeles kings\nGolder answer: Los Angeles Kings\n(F1: 100.00%  |  EM: Yes)\n\nQ55. What company was formed by the merger of Sky Television and British Satellite Broadcasting?\nPredicted answer: Bskyb\nGolder answer: BSkyB\n(F1: 100.00%  |  EM: Yes)\n\nQ56. What satellite enabled Sky Digital to launch an all new digital service?\nPredicted answer: Eurobird 1\nGolder answer: Eutelsat's Eurobird 1\n(F1: 66.67%  |  EM: No)\n\nQ57. What were NTL's services rebranded as?\nPredicted answer: Virgin media\nGolder answer: Virgin Media\n(F1: 100.00%  |  EM: Yes)\n\nQ58. When were the talks held for braodcast right to the Primier league for a five year period from the 1992 season?\nPredicted answer: Autumn of 1991\nGolder answer: 1991\n(F1: 50.00%  |  EM: No)\n\nQ59. In what direction does the mountain system extend?\nPredicted answer: East west\nGolder answer: east-west\n(F1: 100.00%  |  EM: Yes)\n\nQ60. What state in Australia is the center of dairy farming?\nPredicted answer: Victoria\nGolder answer: Victoria\n(F1: 100.00%  |  EM: Yes)\n\nQ61. Where do other tourist events happen in Victoria outside of Melbourne?\nPredicted answer: Regional cities\nGolder answer: regional cities\n(F1: 100.00%  |  EM: Yes)\n\nQ62. Where in South Carolina did Huguenot nobility settle?\nPredicted answer: Charleston\nGolder answer: the Charleston Orange district\n(F1: 50.00%  |  EM: No)\n\nQ63. Besides Britain and North America, where else did Huguenot refugees settle?\nPredicted answer: New york and virginia\nGolder answer: Holland, Prussia, and South Africa\n(F1: 22.22%  |  EM: No)\n\nQ64. What signer of the Articles of Confederation was descended from Huguenots?\nPredicted answer: Paul revere\nGolder answer: Henry Laurens\n(F1: 0.00%  |  EM: No)\n\nQ65. Where did he begin teaching?\nPredicted answer: Rotterdam\nGolder answer: Rotterdam\n(F1: 100.00%  |  EM: Yes)\n\nQ66. What London neighborhood attracted Huguenot refugees?\nPredicted answer: Shoreditch\nGolder answer: Shoreditch\n(F1: 100.00%  |  EM: Yes)\n\nQ67. About what year was the atmospheric engine invented?\nPredicted answer: 1712\nGolder answer: 1712\n(F1: 100.00%  |  EM: Yes)\n\nQ68. Along with Stephenson and Walschaerts, what is an example of a simple motion?\nPredicted answer: Joy\nGolder answer: Joy\n(F1: 100.00%  |  EM: Yes)\n\nQ69. How many pounds of steam per kilowatt hour does the Energiprojekt AB engine use?\nPredicted answer: 8 8\nGolder answer: 8.8\n(F1: 50.00%  |  EM: Yes)\n\nQ70. What was the title of Philo's work?\nPredicted answer: Pneumatica\nGolder answer: Pneumatica\n(F1: 100.00%  |  EM: Yes)\n\nQ71. What does the transport and storage demand for safety in dealing with oxygen?\nPredicted answer: Gaseous and liquid oxygen will act as fuel\nGolder answer: special training\n(F1: 0.00%  |  EM: No)\n\nQ72. What second part of air was deemed lifeless by Lavoisier?\nPredicted answer: Trapped air had been consumed he also noted that tin had increased in weight and that increase was same as weight of air that rushed back in this and other experiments on combustion were documented in his book sur la combustion en general which was published in 1777 in that work he proved that air is mixture of two gases vital air which is essential to combustion and respiration and azote\nGolder answer: azote\n(F1: 2.78%  |  EM: No)\n\nQ73. What does photosynthesis release into the Earth's atmosphere?\nPredicted answer: Oxygen\nGolder answer: oxygen\n(F1: 100.00%  |  EM: Yes)\n\nQ74. What is the speed limit set to reduce consumption?\nPredicted answer: 55 mph\nGolder answer: 55 mph\n(F1: 100.00%  |  EM: Yes)\n\nQ75. What did Mitsubishi rename its Forte to?\nPredicted answer: Dodge d 50\nGolder answer: Dodge D-50\n(F1: 100.00%  |  EM: Yes)\n\nQ76. By which year did Chrysler ended its full sized luxury model?\nPredicted answer: 1981\nGolder answer: 1981\n(F1: 100.00%  |  EM: Yes)\n\nQ77. What was the general consensus Johnson came to regarding America's progress on going to space and reaching a position of leadership?\nPredicted answer: We are neither making maximum effort nor achieving results necessary if this country is to reach position of leadership\nGolder answer: neither making maximum effort nor achieving results necessary\n(F1: 59.26%  |  EM: No)\n\nQ78. In what state were the original launching facilities for missions?\nPredicted answer: Florida\nGolder answer: Florida\n(F1: 100.00%  |  EM: Yes)\n\nQ79. How much did the CM weigh in kgs?\nPredicted answer: 12 250 pounds 5 560\nGolder answer: 5,560 kg\n(F1: 50.00%  |  EM: No)\n\nQ80. What did the second stage in the Saturn V end up doing?\nPredicted answer: Burned liquid hydrogen\nGolder answer: burned liquid hydrogen\n(F1: 100.00%  |  EM: Yes)\n\nQ81. Where did Apollo 1's crew conduct tests at Kennedy Space Center?\nPredicted answer: Altitude chamber\nGolder answer: altitude chamber\n(F1: 100.00%  |  EM: Yes)\n\nQ82. On what day were images of the moon's surface transmitted to Earth via television images?\nPredicted answer: Christmas eve\nGolder answer: Christmas Eve\n(F1: 100.00%  |  EM: Yes)\n\nQ83. When do the stated Treaties apply?\nPredicted answer: As soon as they enter into force\nGolder answer: Treaties apply as soon as they enter into force, unless stated otherwise\n(F1: 63.16%  |  EM: No)\n\nQ84. What happens first if a Directive's deadline for implementation is not met?\nPredicted answer: Member state cannot enforce conflicting laws\nGolder answer: the member state cannot enforce conflicting laws, and a citizen may rely on the Directive in such an action\n(F1: 57.14%  |  EM: No)\n\nQ85. When was the Lisbon Treaty established?\nPredicted answer: 2007\nGolder answer: 2007\n(F1: 100.00%  |  EM: Yes)\n\nQ86. How many pieces of legislation has the Social Charter become the basis for?\nPredicted answer: 40\nGolder answer: 40\n(F1: 100.00%  |  EM: Yes)\n\nQ87. What language did the Court of Justice accept to be required to teach in a Dublin college in Groner v Minister for Education?\nPredicted answer: Gaelic\nGolder answer: Gaelic\n(F1: 100.00%  |  EM: Yes)\n\nQ88. What is the lake known as which was created by the rise of the Andes Mountains?\nPredicted answer: Solimoes basin\nGolder answer: Solimões Basin\n(F1: 50.00%  |  EM: No)\n\nQ89. Scientists disagree with how the Amazon rainforest changed over time with some arguing that it was reduced to isolated refugia seperated by what?\nPredicted answer: Open forest and grassland\nGolder answer: open forest and grassland\n(F1: 100.00%  |  EM: Yes)\n\nQ90. What is terra preta called?\nPredicted answer: Black earth\nGolder answer: black earth\n(F1: 100.00%  |  EM: Yes)\n\nQ91. Which group has young that are born with no tentacles and a large mouth?\nPredicted answer: Beroids\nGolder answer: beroids\n(F1: 100.00%  |  EM: Yes)\n\nQ92. What event was blamed on the introduction of mnemiopsis into The Black Sea?\nPredicted answer: Fish stocks\nGolder answer: causing fish stocks to collapse\n(F1: 57.14%  |  EM: No)\n\nQ93. Ciliary rosettes pump water into what to control buoyancy?\nPredicted answer: Mesoglea\nGolder answer: the mesoglea\n(F1: 100.00%  |  EM: Yes)\n\nQ94. Between which two streets along Kearney Boulevard were wealthy African-Americans at one time residing?\nPredicted answer: Fresno street and thorne ave\nGolder answer: Fresno Street and Thorne Ave\n(F1: 100.00%  |  EM: Yes)\n\nQ95. How many Native American people resided in Fresno in 2010?\nPredicted answer: 8 525\nGolder answer: 8,525\n(F1: 100.00%  |  EM: Yes)\n\nQ96. What 3 things does the Air Force work key on \nPredicted answer: Use of decentralized network with multiple paths between any two points dividing user messages into message blocks later called packets and delivery of these messages by store and forward switching\nGolder answer: use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks\n(F1: 72.34%  |  EM: No)\n\nQ97. What is a connection identifier \nPredicted answer: Address information\nGolder answer: a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking\n(F1: 16.67%  |  EM: No)\n\nQ98. How many people would die of plague in largely populated cities?\nPredicted answer: \nGolder answer: as much as 50%\n(F1: 0.00%  |  EM: No)\n\nQ99. During which years was the plague present in Islamic countries?\nPredicted answer: 1500 and 1850\nGolder answer: between 1500 and 1850\n(F1: 85.71%  |  EM: No)\n\nQ100. What is another word for the Earth's upper mantle?\nPredicted answer: Asthenosphere\nGolder answer: asthenosphere\n(F1: 100.00%  |  EM: Yes)\n\nQ101. Where can the entire sedimentary sequence of the Grand Canyon be seen in less than the length of a meter?\nPredicted answer: Maria fold and thrust belt\nGolder answer: within the Maria Fold and Thrust Belt\n(F1: 90.91%  |  EM: No)\n\nQ102. How is the opening of the Grainger Market documented in the Laing Art Gallery?\nPredicted answer: Painting\nGolder answer: a painting\n(F1: 100.00%  |  EM: Yes)\n\nQ103. What type of impact can the residents of Newcastle expect the city's noise to have on them?\nPredicted answer: Negative\nGolder answer: negative\n(F1: 100.00%  |  EM: Yes)\n\nQ104. What is the largest independent library outside of London?\nPredicted answer: Literary and philosophical society of newcastle upon tyne\nGolder answer: The Literary and Philosophical Society of Newcastle\n(F1: 85.71%  |  EM: No)\n\nQ105. What festival takes place in April in Newcastle?\nPredicted answer: Newcastle beer festival\nGolder answer: The Newcastle Beer Festival\n(F1: 100.00%  |  EM: Yes)\n\nQ106. What is the largest co-ed independent school in Newcastle?\nPredicted answer: Royal grammar school\nGolder answer: the Royal Grammar School\n(F1: 100.00%  |  EM: Yes)\n\nQ107. What is Michael Carrick and Alan Shearer's profession?\nPredicted answer: Footballers\nGolder answer: international footballers\n(F1: 66.67%  |  EM: No)\n\nQ108. Where was the V&A transferred to from its original location at Marlborough House?\nPredicted answer: Somerset house\nGolder answer: Somerset House\n(F1: 100.00%  |  EM: Yes)\n\nQ109. Who designed the  Möllendorff Dinner Service?\nPredicted answer: Frederick ii great\nGolder answer: Frederick II the Great\n(F1: 100.00%  |  EM: Yes)\n\nQ110. Which part of the V&A collection did the Salting Bequest enhanced?\nPredicted answer: Kakiemon ware\nGolder answer: Chinese and Japanese ceramics\n(F1: 0.00%  |  EM: No)\n\nQ111. In 2007, what company purchased ABC Radio properties?\nPredicted answer: Citadel broadcasting\nGolder answer: Citadel Broadcasting\n(F1: 100.00%  |  EM: Yes)\n\nQ112. ABC carries weekend events for what extreme sports competition?\nPredicted answer: Espn sports saturday block on saturday late afternoons featuring various espn produced documentaries and on sundays either encores of primetime reality series cancelled series being burned off that had no room on primetime schedule occasional theatrical films which were acquired by network in early to mid 2000s that no longer have primetime slot to air in or more recently figure skating and gymnastics specials supplied by disson skating when no sports telecasts are scheduled usually airing between 4 00 and 6 00 p m eastern and pacific time during summer abc airs espn produced highlight compilation programs for open championship golf and wimbledon tennis tournaments\nGolder answer: X Games\n(F1: 0.00%  |  EM: No)\n\nQ113. What year did ABC's \"TGIF\" end?\nPredicted answer: 2000\nGolder answer: 2000\n(F1: 100.00%  |  EM: Yes)\n\nQ114. What Aaron Sorkin created show did ABC debut in 1998?\nPredicted answer: \nGolder answer: Sports Night\n(F1: 0.00%  |  EM: No)\n\nQ115. The 1970s allowed which network to move in to first place in the ratings?\nPredicted answer: Cbs and nbc\nGolder answer: ABC\n(F1: 0.00%  |  EM: No)\n\nQ116. How many seasons did The Love Boat run for?\nPredicted answer: Nine\nGolder answer: nine seasons\n(F1: 66.67%  |  EM: No)\n\nQ117. After the merger of Disney-ABC Television group, ABC Studios, and ABC Entertainment, what was the resulting entity named?\nPredicted answer: Abc entertainment group\nGolder answer: ABC Entertainment Group\n(F1: 100.00%  |  EM: Yes)\n\nQ118. What was ABC's logo based on after the ABC-UPT merger was finalized?\nPredicted answer: Seal of federal communications commission\nGolder answer: the seal of the Federal Communications Commission\n(F1: 100.00%  |  EM: Yes)\n\nQ119. What anitrust regulator had doubts about the ITT and ABC merger?\nPredicted answer: Department of justice\nGolder answer: Donald F. Turner\n(F1: 0.00%  |  EM: No)\n\nQ120. The merger between ITT and ABC was suspended after a complaint was filed by whom in July 1967?\nPredicted answer: Department of justice\nGolder answer: Department of Justice\n(F1: 100.00%  |  EM: Yes)\n\nQ121. Where was ABC Marine World opened?\nPredicted answer: Redwood city california\nGolder answer: Redwood City, California\n(F1: 100.00%  |  EM: Yes)\n\nQ122. What magazine criticized ABC's programming strategy in May 1961?\nPredicted answer: Life\nGolder answer: Life\n(F1: 100.00%  |  EM: Yes)\n\nQ123. What feature of the Shah's army enable the weary Mongol forces easy early victories?\nPredicted answer: Superior strategy and tactics\nGolder answer: fragmentation\n(F1: 0.00%  |  EM: No)\n\nQ124. What river is near Genghis Khan's likely place of burial?\nPredicted answer: Onon river\nGolder answer: Onon River\n(F1: 100.00%  |  EM: Yes)\n\nQ125. What types of pharmacy functions have begun to be outsourced?\nPredicted answer: High risk preparations\nGolder answer: high risk preparations and some other compounding functions\n(F1: 54.55%  |  EM: No)\n\nQ126. What entities are included in the federal health care system?\nPredicted answer: Va indian health service and nih\nGolder answer: the VA, the Indian Health Service, and NIH\n(F1: 100.00%  |  EM: Yes)\n\nQ127. Which provinces in Canada limit the rights of pharmacists in prescribing?\nPredicted answer: Alberta and british columbia\nGolder answer: Alberta and British Columbia\n(F1: 100.00%  |  EM: Yes)\n\nQ128. What are one of the key cell types of the adaptive immune system?\nPredicted answer: T cells\nGolder answer: T cells\n(F1: 100.00%  |  EM: Yes)\n\nQ129. What play showed an early depiction of civil disobedience?\nPredicted answer: Antigone\nGolder answer: Antigone\n(F1: 100.00%  |  EM: Yes)\n\nQ130. When would a person be considered to be excising a constitutional impasse?\nPredicted answer: \nGolder answer: the head of government would be acting in her or his capacity as public official\n(F1: 0.00%  |  EM: No)\n\nQ131. What did Thoreau ask a public figure the taxman to do?\nPredicted answer: \nGolder answer: Resign\n(F1: 0.00%  |  EM: No)\n\nQ132. Thoreau mentions what type of person could corrupt a government system?\nPredicted answer: \nGolder answer: elite politicians\n(F1: 0.00%  |  EM: No)\n\nQ133. What reason is given that you should also protest public companies?\nPredicted answer: Breaches of law\nGolder answer: a larger challenge to the legal system\n(F1: 0.00%  |  EM: No)\n\nQ134. What is the group called that does not agree with government at all?\nPredicted answer: Social contract\nGolder answer: anarchists\n(F1: 0.00%  |  EM: No)\n\nQ135. What is violating a law which is not the goal of the protest called?\nPredicted answer: Indirect civil disobedience\nGolder answer: Indirect civil disobedience\n(F1: 100.00%  |  EM: Yes)\n\nQ136. Agassiz's approach to science combined observation and what?\nPredicted answer: Intuition\nGolder answer: intuition\n(F1: 100.00%  |  EM: Yes)\n\nQ137. What distinction does the Bank of America Tower hold?\nPredicted answer: Tallest building in downtown jacksonville s skyline is bank of america tower constructed in 1990 as barnett center it has height of 617 ft 188 m and includes 42 floors\nGolder answer: tallest building in Downtown Jacksonville\n(F1: 28.57%  |  EM: No)\n\nQ138. What storm had the most significant impact on Jacksonville?\nPredicted answer: Hurricane dora\nGolder answer: Hurricane Dora\n(F1: 100.00%  |  EM: Yes)\n\nQ139. How many residents were recorded in the 2010 census of Jacksonville?\nPredicted answer: 25 033\nGolder answer: 821,784\n(F1: 0.00%  |  EM: No)\n\nQ140. What do capitalist firms substitute equipment for in a Marxian analysis?\nPredicted answer: Labor inputs\nGolder answer: labor inputs\n(F1: 100.00%  |  EM: Yes)\n\nQ141. What is the profession of Jake Rosenfield?\nPredicted answer: Sociologist\nGolder answer: Sociologist\n(F1: 100.00%  |  EM: Yes)\n\nQ142. What type of skills does the market bid up compensation for?\nPredicted answer: Rare\nGolder answer: rare and desired\n(F1: 50.00%  |  EM: No)\n\nQ143. What is negatively correlated to the duration of economic growth?\nPredicted answer: Inequality in wealth and income\nGolder answer: inequality in wealth and income\n(F1: 100.00%  |  EM: Yes)\n\nQ144. Who made early colour videos of the show?\nPredicted answer: Fans\nGolder answer: fans\n(F1: 100.00%  |  EM: Yes)\n\nQ145. In what year did the film also mention the number of regenerations?\nPredicted answer: 1996\nGolder answer: 1996\n(F1: 100.00%  |  EM: Yes)\n\nQ146. During what decade did the campus start to look more modern?\nPredicted answer: 1940s\nGolder answer: the 1940s\n(F1: 100.00%  |  EM: Yes)\n\nQ147. In the fall quarter of 2014, how many students signed up for the university's four graduate divisions?\nPredicted answer: 3 468\nGolder answer: 3,468\n(F1: 100.00%  |  EM: Yes)\n\nQ148. Who did the Han Chinese want to help the Mongols fight?\nPredicted answer: Jin\nGolder answer: the Jin\n(F1: 100.00%  |  EM: Yes)\n\nQ149. Where did the Song dynasty continue to cause problems for Kublai?\nPredicted answer: South\nGolder answer: south\n(F1: 100.00%  |  EM: Yes)\n\nQ150. When did the Jin dynasty end?\nPredicted answer: 1115 – 1234\nGolder answer: 1234\n(F1: 50.00%  |  EM: No)\n\nQ151. What did 'Da Yuan Tong Zhi' mean?\nPredicted answer: Comprehensive institutions of great yuan\nGolder answer: \"the comprehensive institutions of the Great Yuan\"\n(F1: 100.00%  |  EM: Yes)\n\nQ152. What was Kublai's favorite sect of Tibetan Buddhism?\nPredicted answer: Sakya\nGolder answer: Sakya\n(F1: 100.00%  |  EM: Yes)\n\nQ153. What philosophies underlay Chinese medicine?\nPredicted answer: Yin yang and wuxing philosophy\nGolder answer: yin-yang and wuxing\n(F1: 88.89%  |  EM: No)\n\nQ154. What was the second meaning of a Chinese word for 'barracks'?\nPredicted answer: Thanks\nGolder answer: thanks\n(F1: 100.00%  |  EM: Yes)\n\nQ155. Who governed the Central Region in the Yuan?\nPredicted answer: Central secretariat or zhongshu sheng\nGolder answer: the Central Secretariat\n(F1: 57.14%  |  EM: No)\n\nQ156. What did Kenya name itself on December 12, 1964? \nPredicted answer: Republic of kenya\nGolder answer: Republic of Kenya\n(F1: 100.00%  |  EM: Yes)\n\nQ157. What is the second largest contrubtor to Kenyas GDP?\nPredicted answer: Agriculture\nGolder answer: Agriculture\n(F1: 100.00%  |  EM: Yes)\n\nQ158. What system was adopted for education?\nPredicted answer: 7 – 4 – 2 – 3 system\nGolder answer: the 7–4–2–3 system was adopted\n(F1: 16.67%  |  EM: No)\n\nQ159. When was the Special Report on Managing Risks of Extreme Events and Disasters to Advance Climate Change Adaptation (SREX) issued?\nPredicted answer: 2011\nGolder answer: 2011\n(F1: 100.00%  |  EM: Yes)\n\nQ160. Who said Barton's investigation was \"misguided and illegitimate\"?\nPredicted answer: Sherwood boehlert\nGolder answer: Sherwood Boehlert\n(F1: 100.00%  |  EM: Yes)\n\nQ161. What are chloroplasts descended from?\nPredicted answer: Cyanobacteria\nGolder answer: Cyanobacteria\n(F1: 100.00%  |  EM: Yes)\n\nQ162. What kind of cell wall do cyanobacteria have?\nPredicted answer: Peptidoglycan\nGolder answer: peptidoglycan\n(F1: 100.00%  |  EM: Yes)\n\nQ163. What makes red algae red?\nPredicted answer: Phycobilin phycoerytherin is responsible for giving many red algae their distinctive red color however since they also contain blue green chlorophyll and other pigments many are reddish to purple from combination\nGolder answer: the phycobilin phycoerytherin\n(F1: 12.12%  |  EM: No)\n\nQ164. What lineage is Karlodinium in?\nPredicted answer: Fucoxanthin dinophyte\nGolder answer: fucoxanthin dinophyte\n(F1: 100.00%  |  EM: Yes)\n\nQ165. What is a single Plastoglobuli called?\nPredicted answer: Plastoglobulus\nGolder answer: plastoglobulus, sometimes spelled plastoglobule(s)\n(F1: 33.33%  |  EM: No)\n\nQ166. What cycle starts with rubisco?\nPredicted answer: Calvin cycle\nGolder answer: The Calvin cycle\n(F1: 100.00%  |  EM: Yes)\n\nQ167. What is unusual about C4 plants' chloroplasts?\nPredicted answer: Distinct chloroplast dimorphism\nGolder answer: they exhibit a distinct chloroplast dimorphism\n(F1: 75.00%  |  EM: No)\n\nQ168. Of what form do Mersenne primes take?\nPredicted answer: 2p − 1\nGolder answer: 2p − 1\n(F1: 100.00%  |  EM: Yes)\n\nQ169. When using a probabilistic algorithm, how is the probability that the number is composite expressed mathematically?\nPredicted answer: 1 1 p n\nGolder answer: 1/(1-p)n\n(F1: 75.00%  |  EM: Yes)\n\nQ170. What does the Riemann hypothesis state the source of irregularity in the distribution of points comes from?\nPredicted answer: Random noise\nGolder answer: random noise\n(F1: 100.00%  |  EM: Yes)\n\nQ171. What form do complex Gaussian integers have? \nPredicted answer: Bi\nGolder answer: a + bi\n(F1: 100.00%  |  EM: Yes)\n\nQ172. What happened to the rate of flow in the Rhine during the Rhine straightening program?\nPredicted answer: Increased\nGolder answer: increased\n(F1: 100.00%  |  EM: Yes)\n\nQ173. When did the last glacial start?\nPredicted answer: Pleistocene\nGolder answer: 74,000 (BP\n(F1: 0.00%  |  EM: No)\n\nQ174. Which direction did the Rhine flow during the last cold phase?\nPredicted answer: West\nGolder answer: west\n(F1: 100.00%  |  EM: Yes)\n\nQ175. In May 2002, where would you go to address the Parliament?\nPredicted answer: University of aberdeen\nGolder answer: University of Aberdeen\n(F1: 100.00%  |  EM: Yes)\n\nQ176. Which hall was used as Parliament's principle committee room?\nPredicted answer: Main hall\nGolder answer: main\n(F1: 66.67%  |  EM: No)\n\nQ177. Who is elected to serve as the Presiding Officer at the beginning of each parliamentary session?\nPredicted answer: One msp\nGolder answer: one MSP\n(F1: 100.00%  |  EM: Yes)\n\nQ178. How many seats must a political party have to be represented on the Parliamentary Bureau?\nPredicted answer: Five\nGolder answer: five\n(F1: 100.00%  |  EM: Yes)\n\nQ179. Who decides how land or property is allowed to be used?\nPredicted answer: \nGolder answer: Scottish Government.\n(F1: 0.00%  |  EM: No)\n\nQ180. How much can the SP alter income tax in Scotland?\nPredicted answer: Up to 3 pence in pound\nGolder answer: up to 3 pence in the pound\n(F1: 100.00%  |  EM: Yes)\n\nQ181. How is the process of allocating seats repeated until all available seats have been determined?\nPredicted answer: Iteratively\nGolder answer: iteratively\n(F1: 100.00%  |  EM: Yes)\n\nQ182. How did Turabi build a strong economic base?\nPredicted answer: With money from foreign islamist banking systems\nGolder answer: money from foreign Islamist banking systems\n(F1: 92.31%  |  EM: No)\n\nQ183. The West saw themselves as what compared to the east?\nPredicted answer: \nGolder answer: progressive\n(F1: 0.00%  |  EM: No)\n\nQ184. How did france differ from Britain in managing its colonies?\nPredicted answer: Contrasting from britain france sent small numbers of settlers to its colonies with only notable exception of algeria where french settlers nevertheless always remained small minority\nGolder answer: small numbers of settlers\n(F1: 26.67%  |  EM: No)\n\nQ185. When was Otto von Bismarck born?\nPredicted answer: 1862 – 90\nGolder answer: 1862\n(F1: 50.00%  |  EM: No)\n\nQ186. Who besides Woodrow Wilson himself had the idea for the inquiry?\nPredicted answer: President wilson and american delegation from paris peace conference\nGolder answer: American delegation from the Paris Peace Conference\n(F1: 80.00%  |  EM: No)\n\nQ187. When was the UMC founded?\nPredicted answer: 1968\nGolder answer: 1968\n(F1: 100.00%  |  EM: Yes)\n\nQ188. What is the main purpose of the jurisdictions and central conferences?\nPredicted answer: To elect and appoint bishops chief administrators of church\nGolder answer: to elect and appoint bishops\n(F1: 71.43%  |  EM: No)\n\nQ189. No appointment is official fixed until what occurs?\nPredicted answer: Bishop has read appointments at session of annual conference\nGolder answer: bishop has read the appointments at the session of the Annual Conference\n(F1: 100.00%  |  EM: Yes)\n\nQ190. By the opening of the 2008 General Conference, what was the total UMC membership in the U.S.?\nPredicted answer: 11 4 million\nGolder answer: 7.9 million\n(F1: 33.33%  |  EM: No)\n\nQ191. By the opening of the 2008 General Conference, what was the total UMC membership overseas?\nPredicted answer: 11 4 million with about 7 9 million in u s and 3 5 million\nGolder answer: 3.5 million\n(F1: 33.33%  |  EM: No)\n\nQ192. Was the plan formalized?\nPredicted answer: Goal of congress was to formalize unified front in trade and negotiations with various indians since allegiance of various tribes and nations was seen to be pivotal in success in war that was unfolding plan that delegates agreed to was never ratified by colonial legislatures nor approved of by crown\nGolder answer: The plan that the delegates agreed to was never ratified by the colonial legislatures nor approved of by the crown\n(F1: 45.45%  |  EM: No)\n\nQ193. In 1758 what was duc de Choiseul's plan for focused military efforts?\nPredicted answer: Invasion of britain\nGolder answer: invasion of Britain, to draw British resources away from North America and the European mainland\n(F1: 35.29%  |  EM: No)\n\nQ194. How successful was the French revised efforts?\nPredicted answer: Invasion failed both militarily and politically\nGolder answer: The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France\n(F1: 57.14%  |  EM: No)\n\nQ195. In Sept 1760 who negotiated a capitulation from Montreal?\nPredicted answer: General amherst\nGolder answer: Governor Vaudreuil\n(F1: 0.00%  |  EM: No)\n\nQ196. What was the belief that maintaining motion required force?\nPredicted answer: Constant velocity\nGolder answer: fundamental error\n(F1: 0.00%  |  EM: No)\n\nQ197. Who experimented by rolling stones and canonballs down a steep incline?\nPredicted answer: Galileo\nGolder answer: Galileo\n(F1: 100.00%  |  EM: Yes)\n\nQ198. How fast do objects fall on Earth?\nPredicted answer: 9 81 meters per second squared\nGolder answer: about 9.81 meters per second squared\n(F1: 92.31%  |  EM: No)\n\nQ199. What was dificult to reconcile the photoelectric effect and the missing ultraviolet catastrophe?\nPredicted answer: Nonexistence of ultraviolet catastrophe proved troublesome\nGolder answer: electromagnetic theory\n(F1: 0.00%  |  EM: No)\n\nQ200. What is often misunderstood as the cause of matter rigidity?\nPredicted answer: Electromagnetic force\nGolder answer: repulsion of like charges\n(F1: 0.00%  |  EM: No)\n\n","output_type":"stream"},{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"./\n  __notebook_source__.ipynb\n./results/\n  Train_max_lens_Csum_maxLen50.png\n  Train_max_lens_Csum_maxLen3263.png\n  Train_max_lens_Csum_maxLen100.png\n  Dev_max_lens_maxLen150.png\n  Train_max_lens_maxLen50.png\n  Dev_max_lens_maxLen512.png\n  Dev_max_lens_Csum_maxLen512.png\n  Dev_max_lens_Csum_maxLen792.png\n  Train_200_questions.txt\n  Dev_max_lens_Csum.png\n  Train_max_lens_Csum.png\n  Dev_max_lens_Csum_maxLen50.png\n  Train_max_lens_Csum_maxLen512.png\n  Train_max_lens_maxLen150.png\n  Dev_max_lens_maxLen50.png\n  Train_max_lens_maxLen200.png\n  Dev_max_lens_Csum_maxLen150.png\n  Train_max_lens_Csum_maxLen150.png\n  Dev_max_lens_maxLen200.png\n  Train_max_lens_maxLen256.png\n  Train_max_lens_maxLen512.png\n  Train_max_lens_maxLen792.png\n  Train_max_lens_Csum_maxLen256.png\n  Dev_max_lens_maxLen100.png\n  Dev_max_lens.png\n  Dev_max_lens_maxLen792.png\n  Train_max_lens_Csum_maxLen300.png\n  Train_max_lens_maxLen3263.png\n  Dev_max_lens_Csum_maxLen300.png\n  Dev_max_lens_maxLen256.png\n  Train_max_lens_maxLen100.png\n  Train_max_lens_Csum_maxLen200.png\n  Dev_max_lens_Csum_maxLen200.png\n  Train_max_lens_maxLen300.png\n  Dev_max_lens_Csum_maxLen256.png\n  Dev_max_lens_maxLen300.png\n  Dev_max_lens_Csum_maxLen100.png\n  Dev_max_lens_Csum_maxLen3263.png\n  Dev_max_lens_maxLen3263.png\n  Train_max_lens.png\n  Train_max_lens_Csum_maxLen792.png","text/html":"./<br>\n&nbsp;&nbsp;<a href='./__notebook_source__.ipynb' target='_blank'>__notebook_source__.ipynb</a><br>\n./results/<br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen50.png' target='_blank'>Train_max_lens_Csum_maxLen50.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen3263.png' target='_blank'>Train_max_lens_Csum_maxLen3263.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen100.png' target='_blank'>Train_max_lens_Csum_maxLen100.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen150.png' target='_blank'>Dev_max_lens_maxLen150.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen50.png' target='_blank'>Train_max_lens_maxLen50.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen512.png' target='_blank'>Dev_max_lens_maxLen512.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen512.png' target='_blank'>Dev_max_lens_Csum_maxLen512.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen792.png' target='_blank'>Dev_max_lens_Csum_maxLen792.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_200_questions.txt' target='_blank'>Train_200_questions.txt</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum.png' target='_blank'>Dev_max_lens_Csum.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum.png' target='_blank'>Train_max_lens_Csum.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen50.png' target='_blank'>Dev_max_lens_Csum_maxLen50.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen512.png' target='_blank'>Train_max_lens_Csum_maxLen512.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen150.png' target='_blank'>Train_max_lens_maxLen150.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen50.png' target='_blank'>Dev_max_lens_maxLen50.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen200.png' target='_blank'>Train_max_lens_maxLen200.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen150.png' target='_blank'>Dev_max_lens_Csum_maxLen150.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen150.png' target='_blank'>Train_max_lens_Csum_maxLen150.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen200.png' target='_blank'>Dev_max_lens_maxLen200.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen256.png' target='_blank'>Train_max_lens_maxLen256.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen512.png' target='_blank'>Train_max_lens_maxLen512.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen792.png' target='_blank'>Train_max_lens_maxLen792.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen256.png' target='_blank'>Train_max_lens_Csum_maxLen256.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen100.png' target='_blank'>Dev_max_lens_maxLen100.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens.png' target='_blank'>Dev_max_lens.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen792.png' target='_blank'>Dev_max_lens_maxLen792.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen300.png' target='_blank'>Train_max_lens_Csum_maxLen300.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen3263.png' target='_blank'>Train_max_lens_maxLen3263.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen300.png' target='_blank'>Dev_max_lens_Csum_maxLen300.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen256.png' target='_blank'>Dev_max_lens_maxLen256.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen100.png' target='_blank'>Train_max_lens_maxLen100.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen200.png' target='_blank'>Train_max_lens_Csum_maxLen200.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen200.png' target='_blank'>Dev_max_lens_Csum_maxLen200.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_maxLen300.png' target='_blank'>Train_max_lens_maxLen300.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen256.png' target='_blank'>Dev_max_lens_Csum_maxLen256.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen300.png' target='_blank'>Dev_max_lens_maxLen300.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen100.png' target='_blank'>Dev_max_lens_Csum_maxLen100.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_Csum_maxLen3263.png' target='_blank'>Dev_max_lens_Csum_maxLen3263.png</a><br>\n&nbsp;&nbsp;<a href='./results/Dev_max_lens_maxLen3263.png' target='_blank'>Dev_max_lens_maxLen3263.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens.png' target='_blank'>Train_max_lens.png</a><br>\n&nbsp;&nbsp;<a href='./results/Train_max_lens_Csum_maxLen792.png' target='_blank'>Train_max_lens_Csum_maxLen792.png</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 📚 References\n\n[SQuAD: 100,000+ Questions for Machine Comprehension of Tex](https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf)<br>\n[Evaluating QA: Metrics, Predictions, and the Null Response](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/2020-06-09-Evaluating_BERT_on_SQuAD.ipynb)<br>\n[How-to Fine-Tune a Q&A Transformer](https://towardsdatascience.com/how-to-fine-tune-a-q-a-transformer-86f91ec92997)<br>\n[NLP - Document Retrieval for Question Answering](https://www.kaggle.com/code/leomauro/nlp-document-retrieval-for-question-answering)<br>\n[DistilBERT base model (uncased) ](https://huggingface.co/distilbert-base-uncased)<br>\n[SQuAD v1.1](https://datarepository.wolframcloud.com/resources/SQuAD-v1.1)<br>\n[kamalkraj/BERT-SQuAD ](https://github.com/kamalkraj/BERT-SQuAD)<br>\n[Question answering](https://huggingface.co/docs/transformers/tasks/question_answering)<br>\n\n\n\n\n","metadata":{}}]}